{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SageMaker Distributed Model Parallel with Amazon SageMaker to Launch an MNIST Training Job with Model Parallelization\n",
    "\n",
    "SageMaker Distributed Model Parallel (SMP) is a model parallelism library for training large deep learning models that were previously difficult to train due to GPU memory limitations. SageMaker Distributed Model Parallel automatically and efficiently splits a model across multiple GPUs and instances and coordinates model training, allowing you to increase prediction accuracy by creating larger models with more parameters.\n",
    "\n",
    "Use this notebook to configure Sagemaker Distributed Model Parallel to train a model using an example PyTorch training script, `utils/pt_mnist.py` and [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#train-a-model-with-the-sagemaker-python-sdk). \n",
    "\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to learn more about SMP and using SageMaker with Pytorch. \n",
    "\n",
    "* To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](http://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "* To learn more about using the SageMaker Python SDK with Pytorch, see [Using PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "* To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Run the following cells to initialize the notebook instance. Get the SageMaker execution role used to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.25)\n",
      "Requirement already satisfied: boto3>=1.16.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-experiments) (1.16.37)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.19.37)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.16.27->sagemaker-experiments) (2.8.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.16.27->sagemaker-experiments) (1.25.11)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.37->boto3>=1.16.27->sagemaker-experiments) (1.15.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.19.37)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.23.4.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (20.7)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (3.14.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (3.1.0)\n",
      "Requirement already satisfied: boto3>=1.16.32 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.16.37)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.19.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (1.19.37)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.16.32->sagemaker) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.16.32->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-pasta->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-pasta->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-pasta->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (3.14.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-pasta->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (1.19.37)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role:arn:aws:iam::230755935769:role/SageMakerExecutionRoleMLOps\n",
      "CPU times: user 1.01 s, sys: 427 ms, total: 1.44 s\n",
      "Wall time: 938 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "role = get_execution_role() # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f'SageMaker Execution Role:{role}')\n",
    "\n",
    "session = boto3.session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your training script\n",
    "\n",
    "Run the following cell to view an example-training script you will use in this demo. This is a PyTorch 1.6 trianing script that uses the MNIST dataset. \n",
    "\n",
    "You will see that the script contains `SMP` specific operations and decorators, which configure model parallel training. See the training script comments to learn more about the SMP functions and types used in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Future\r\n",
      "from __future__ import print_function\r\n",
      "\r\n",
      "# Standard Library\r\n",
      "import os, time\r\n",
      "import argparse\r\n",
      "import math\r\n",
      "import random\r\n",
      "\r\n",
      "# Third Party\r\n",
      "import numpy as np\r\n",
      "import torch\r\n",
      "import torch.nn as nn\r\n",
      "import torch.nn.functional as F\r\n",
      "import torch.optim as optim\r\n",
      "from torch.cuda.amp import autocast\r\n",
      "from torch.optim.lr_scheduler import StepLR\r\n",
      "from torchnet.dataset import SplitDataset\r\n",
      "from torchvision import datasets, transforms\r\n",
      "\r\n",
      "# First Party\r\n",
      "import smdistributed.modelparallel.torch as smp\r\n",
      "\r\n",
      "# SM Distributed: import scaler from smdistributed.modelparallel.torch.amp, instead of torch.cuda.amp\r\n",
      "\r\n",
      "# Make cudnn deterministic in order to get the same losses across runs.\r\n",
      "# The following two lines can be removed if they cause a performance impact.\r\n",
      "# For more details, see:\r\n",
      "# https://pytorch.org/docs/stable/notes/randomness.html#cudnn\r\n",
      "torch.backends.cudnn.deterministic = True\r\n",
      "torch.backends.cudnn.benchmark = False\r\n",
      "\r\n",
      "\r\n",
      "def aws_s3_sync(source, destination):\r\n",
      "    \r\n",
      "    \"\"\"aws s3 sync in quiet mode and time profile\"\"\"\r\n",
      "    import time, subprocess\r\n",
      "    cmd = [\"aws\", \"s3\", \"sync\", \"--quiet\", source, destination]\r\n",
      "    print(f\"Syncing files from {source} to {destination}\")\r\n",
      "    start_time = time.time()\r\n",
      "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\r\n",
      "    p.wait()\r\n",
      "    end_time = time.time()\r\n",
      "    print(\"Time Taken to Sync: \", (end_time-start_time))\r\n",
      "    return\r\n",
      "\r\n",
      "def sync_local_checkpoints_to_s3(local_path=\"/opt/ml/checkpoints\", s3_path=os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))+'/checkpoints'):\r\n",
      "    \r\n",
      "    \"\"\" sample function to sync checkpoints from local path to s3 \"\"\"\r\n",
      "\r\n",
      "    import boto3, botocore\r\n",
      "    #check if local path exists\r\n",
      "    if not os.path.exists(local_path):\r\n",
      "        raise RuntimeError(\"Provided local path {local_path} does not exist. Please check\")\r\n",
      "\r\n",
      "    #check if s3 bucket exists\r\n",
      "    s3 = boto3.resource('s3')\r\n",
      "    if 's3://' not in s3_path:\r\n",
      "        raise ValueError(\"Provided s3 path {s3_path} is not valid. Please check\")\r\n",
      "\r\n",
      "    s3_bucket = s3_path.replace('s3://','').split('/')[0]\r\n",
      "    print(f\"S3 Bucket: {s3_bucket}\")\r\n",
      "    try:\r\n",
      "        s3.meta.client.head_bucket(Bucket=s3_bucket)\r\n",
      "    except botocore.exceptions.ClientError as e:\r\n",
      "        error_code = e.response['Error']['Code']\r\n",
      "        if error_code == '404':\r\n",
      "            raise RuntimeError('S3 bucket does not exist. Please check')\r\n",
      "    aws_s3_sync(local_path, s3_path)\r\n",
      "    return\r\n",
      "\r\n",
      "def sync_s3_checkpoints_to_local(local_path=\"/opt/ml/checkpoints\", s3_path=os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))+'/checkpoints'):\r\n",
      "    \r\n",
      "    \"\"\" sample function to sync checkpoints from s3 to local path \"\"\"\r\n",
      "\r\n",
      "    import boto3, botocore\r\n",
      "    #creat if local path does not exists\r\n",
      "    if not os.path.exists(local_path):\r\n",
      "        print(f\"Provided local path {local_path} does not exist. Creating...\")\r\n",
      "        try:\r\n",
      "            os.makedirs(local_path)\r\n",
      "        except Exception as e:\r\n",
      "            raise RuntimeError(f\"failed to create {local_path}\")\r\n",
      "\r\n",
      "    #check if s3 bucket exists\r\n",
      "    s3 = boto3.resource('s3')\r\n",
      "    if 's3://' not in s3_path:\r\n",
      "        raise ValueError(\"Provided s3 path {s3_path} is not valid. Please check\")\r\n",
      "\r\n",
      "    s3_bucket = s3_path.replace('s3://','').split('/')[0]\r\n",
      "    print(f\"S3 Bucket: {s3_bucket}\")\r\n",
      "    try:\r\n",
      "        s3.meta.client.head_bucket(Bucket=s3_bucket)\r\n",
      "    except botocore.exceptions.ClientError as e:\r\n",
      "        error_code = e.response['Error']['Code']\r\n",
      "        if error_code == '404':\r\n",
      "            raise RuntimeError('S3 bucket does not exist. Please check')\r\n",
      "    aws_s3_sync(s3_path, local_path)\r\n",
      "    return\r\n",
      "\r\n",
      "class Net1(nn.Module):\r\n",
      "    def __init__(self):\r\n",
      "        super(Net1, self).__init__()\r\n",
      "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\r\n",
      "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\r\n",
      "\r\n",
      "    def forward(self, x):\r\n",
      "        x = self.conv1(x)\r\n",
      "        x = F.relu(x)\r\n",
      "        x = self.conv2(x)\r\n",
      "        x = F.relu(x)\r\n",
      "        x = F.max_pool2d(x, 2)\r\n",
      "        x = torch.flatten(x, 1)\r\n",
      "        return x\r\n",
      "\r\n",
      "\r\n",
      "class Net2(nn.Module):\r\n",
      "    def __init__(self):\r\n",
      "        super(Net2, self).__init__()\r\n",
      "        self.fc1 = nn.Linear(9216, 128)\r\n",
      "        self.fc2 = nn.Linear(128, 10)\r\n",
      "\r\n",
      "    def forward(self, x):\r\n",
      "        x = self.fc1(x)\r\n",
      "        x = F.relu(x)\r\n",
      "        x = self.fc2(x)\r\n",
      "        output = F.log_softmax(x, 1)\r\n",
      "        return output\r\n",
      "\r\n",
      "\r\n",
      "class GroupedNet(nn.Module):\r\n",
      "    def __init__(self):\r\n",
      "        super(GroupedNet, self).__init__()\r\n",
      "        self.net1 = Net1()\r\n",
      "        self.net2 = Net2()\r\n",
      "\r\n",
      "    def forward(self, x):\r\n",
      "        x = self.net1(x)\r\n",
      "        x = self.net2(x)\r\n",
      "        return x\r\n",
      "\r\n",
      "\r\n",
      "# SM Distributed: Define smp.step. Return any tensors needed outside.\r\n",
      "@smp.step\r\n",
      "def train_step(model, scaler, data, target):\r\n",
      "    with autocast(1 > 0):\r\n",
      "        output = model(data)\r\n",
      "\r\n",
      "    loss = F.nll_loss(output, target, reduction=\"mean\")\r\n",
      "\r\n",
      "    scaled_loss = loss\r\n",
      "    model.backward(scaled_loss)\r\n",
      "    return output, loss\r\n",
      "\r\n",
      "\r\n",
      "def train(model, scaler, device, train_loader, optimizer, epoch):\r\n",
      "    model.train()\r\n",
      "    for batch_idx, (data, target) in enumerate(train_loader):\r\n",
      "        # SM Distributed: Move input tensors to the GPU ID used by the current process,\r\n",
      "        # based on the set_device call.\r\n",
      "        data, target = data.to(device), target.to(device)\r\n",
      "        optimizer.zero_grad()\r\n",
      "        # Return value, loss_mb is a StepOutput object\r\n",
      "        _, loss_mb = train_step(model, scaler, data, target)\r\n",
      "\r\n",
      "        # SM Distributed: Average the loss across microbatches.\r\n",
      "        loss = loss_mb.reduce_mean()\r\n",
      "\r\n",
      "        optimizer.step()\r\n",
      "\r\n",
      "        if smp.rank() == 0 and batch_idx % 10 == 0:\r\n",
      "            print(\r\n",
      "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\r\n",
      "                    epoch,\r\n",
      "                    batch_idx * len(data),\r\n",
      "                    len(train_loader.dataset),\r\n",
      "                    100.0 * batch_idx / len(train_loader),\r\n",
      "                    loss.item(),\r\n",
      "                )\r\n",
      "            )\r\n",
      "\r\n",
      "\r\n",
      "# SM Distributed: Define smp.step for evaluation.\r\n",
      "@smp.step\r\n",
      "def test_step(model, data, target):\r\n",
      "    output = model(data)\r\n",
      "    loss = F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\r\n",
      "    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\r\n",
      "    correct = pred.eq(target.view_as(pred)).sum().item()\r\n",
      "    return loss, correct\r\n",
      "\r\n",
      "\r\n",
      "def test(model, device, test_loader):\r\n",
      "    model.eval()\r\n",
      "    test_loss = 0\r\n",
      "    correct = 0\r\n",
      "    with torch.no_grad():\r\n",
      "        for batch_idx, (data, target) in enumerate(test_loader):\r\n",
      "            # SM Distributed: Moves input tensors to the GPU ID used by the current process\r\n",
      "            # based on the set_device call.\r\n",
      "            data, target = data.to(device), target.to(device)\r\n",
      "\r\n",
      "            # Since test_step returns scalars instead of tensors,\r\n",
      "            # test_step decorated with smp.step will return lists instead of StepOutput objects.\r\n",
      "            loss_batch, correct_batch = test_step(model, data, target)\r\n",
      "            test_loss += sum(loss_batch)\r\n",
      "            correct += sum(correct_batch)\r\n",
      "\r\n",
      "    test_loss /= len(test_loader.dataset)\r\n",
      "    if smp.mp_rank() == 0:\r\n",
      "        print(\r\n",
      "            \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\r\n",
      "                test_loss,\r\n",
      "                correct,\r\n",
      "                len(test_loader.dataset),\r\n",
      "                100.0 * correct / len(test_loader.dataset),\r\n",
      "            )\r\n",
      "        )\r\n",
      "    return test_loss\r\n",
      "\r\n",
      "def main():\r\n",
      "    if not torch.cuda.is_available():\r\n",
      "        raise ValueError(\"The script requires CUDA support, but CUDA not available\")\r\n",
      "    use_ddp = True\r\n",
      "    use_horovod = False\r\n",
      "\r\n",
      "    # Fix seeds in order to get the same losses across runs\r\n",
      "    random.seed(1)\r\n",
      "    np.random.seed(1)\r\n",
      "    torch.manual_seed(1)\r\n",
      "    torch.cuda.manual_seed(1)\r\n",
      "\r\n",
      "    smp.init()\r\n",
      "\r\n",
      "    # SM Distributed: Set the device to the GPU ID used by the current process.\r\n",
      "    # Input tensors should be transferred to this device.\r\n",
      "    torch.cuda.set_device(smp.local_rank())\r\n",
      "    device = torch.device(\"cuda\")\r\n",
      "    kwargs = {\"batch_size\": 64}\r\n",
      "    kwargs.update({\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": False})\r\n",
      "\r\n",
      "    transform = transforms.Compose(\r\n",
      "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\r\n",
      "    )\r\n",
      "\r\n",
      "    # SM Distributed: Download only on a single process per instance.\r\n",
      "    # When this is not present, the file is corrupted by multiple processes trying\r\n",
      "    # to download and extract at the same time\r\n",
      "    if smp.local_rank() == 0:\r\n",
      "        dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\r\n",
      "    smp.barrier()\r\n",
      "    dataset1 = datasets.MNIST(\"../data\", train=True, download=False, transform=transform)\r\n",
      "\r\n",
      "    if (use_ddp or use_horovod) and smp.dp_size() > 1:\r\n",
      "        partitions_dict = {f\"{i}\": 1 / smp.dp_size() for i in range(smp.dp_size())}\r\n",
      "        dataset1 = SplitDataset(dataset1, partitions=partitions_dict)\r\n",
      "        dataset1.select(f\"{smp.dp_rank()}\")\r\n",
      "\r\n",
      "    # Download and create dataloaders for train and test dataset\r\n",
      "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\r\n",
      "\r\n",
      "    train_loader = torch.utils.data.DataLoader(dataset1, **kwargs)\r\n",
      "    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\r\n",
      "\r\n",
      "    model = GroupedNet()\r\n",
      "\r\n",
      "    # SMP handles the transfer of parameters to the right device\r\n",
      "    # and the user doesn't need to call 'model.to' explicitly.\r\n",
      "    # model.to(device)\r\n",
      "    optimizer = optim.Adadelta(model.parameters(), lr=4.0)\r\n",
      "\r\n",
      "    # SM Distributed: Use the DistributedModel container to provide the model\r\n",
      "    # to be partitioned across different ranks. For the rest of the script,\r\n",
      "    # the returned DistributedModel object should be used in place of\r\n",
      "    # the model provided for DistributedModel class instantiation.\r\n",
      "    model = smp.DistributedModel(model)\r\n",
      "    scaler = smp.amp.GradScaler()\r\n",
      "    optimizer = smp.DistributedOptimizer(optimizer)\r\n",
      "\r\n",
      "    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\r\n",
      "    for epoch in range(1, 2):\r\n",
      "        train(model, scaler, device, train_loader, optimizer, epoch)\r\n",
      "        test_loss = test(model, device, test_loader)\r\n",
      "        scheduler.step()\r\n",
      "        \r\n",
      "    if smp.rank() == 0:\r\n",
      "        if os.path.exists('/opt/ml/local_checkpoints'):\r\n",
      "            print(\"-INFO- PATH DO EXIST\")\r\n",
      "        else:\r\n",
      "            os.makedirs('/opt/ml/local_checkpoints')\r\n",
      "            print(\"-INFO- PATH DO NOT EXIST\")\r\n",
      "\r\n",
      "    # Waiting the save checkpoint to be finished before run another allgather_object\r\n",
      "    smp.barrier()\r\n",
      "    \r\n",
      "    if smp.dp_rank() == 0:\r\n",
      "        model_dict = model.local_state_dict()\r\n",
      "        opt_dict = optimizer.local_state_dict()\r\n",
      "        smp.save(\r\n",
      "                {\"model_state_dict\": model_dict, \"optimizer_state_dict\": opt_dict},\r\n",
      "                f\"/opt/ml/local_checkpoints/pt_mnist_checkpoint.pt\",\r\n",
      "                partial=True,\r\n",
      "            )\r\n",
      "    smp.barrier()\r\n",
      "    \r\n",
      "    if smp.local_rank() == 0:\r\n",
      "        print(\"Start syncing\")\r\n",
      "        base_s3_path = os.path.dirname(os.path.dirname(os.getenv('SM_MODULE_DIR', '')))\r\n",
      "        curr_host = os.getenv('SM_CURRENT_HOST')\r\n",
      "        full_s3_path = f'{base_s3_path}/checkpoints/{curr_host}/'\r\n",
      "        sync_local_checkpoints_to_s3(local_path='/opt/ml/local_checkpoints', s3_path=full_s3_path)\r\n",
      "        print(\"Finished syncing\")\r\n",
      "        \r\n",
      "\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    main()"
     ]
    }
   ],
   "source": [
    "# Run this cell to see an example of a training scripts that you can use to configure -\n",
    "# SageMaker Distributed Model Parallel with PyTorch version 1.6\n",
    "!cat utils/pt_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Training Job\n",
    "\n",
    "Next, you will use SageMaker Estimator API to define a SageMaker Training Job. You will use an [`Estimator`](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) to define the number and type of EC2 instances Amazon SageMaker uses for training, as well as the size of the volume attached to those instances. \n",
    "\n",
    "You can update the following:\n",
    "* `processes_per_host`\n",
    "* `entry_point`\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `base_job_name`\n",
    "\n",
    "In addition, you can supply and modify configuration parameters for the SageMaker Distributed Model Parallel library. These parameters will be passed in through the `distributions` argument, as shown below.\n",
    "\n",
    "### Update the Type and Number of EC2 Instances Used\n",
    "\n",
    "Specify your `processes_per_host`. Note that it must be a multiple of your partitions, which by default is 2.\n",
    "\n",
    "The instance type and number of instances you specify in `instance_type` and `instance_count` respectively will determine the number of GPUs Amazon SageMaker uses during training. Explicitly, `instance_type` will determine the number of GPUs on a single instance and that number will be multiplied by `instance_count`. \n",
    "\n",
    "You must specify values for `instance_type` and `instance_count` so that the total number of GPUs available for training is equal to `partitions` in `config` of `smp.init` in your training script. \n",
    "\n",
    "\n",
    "To look up instances types, see [Amazon EC2 Instance Types](https://aws.amazon.com/sagemaker/pricing/).\n",
    "\n",
    "\n",
    "### Uploading Checkpoint During Training or Resuming Checkpoint from Previous Training\n",
    "We also provide a custom way for users to upload checkpoints during training or resume checkpoints from previous training. We have integrated this into our `pt_mnist.py` example script. Please see the functions `aws_s3_sync`, `sync_local_checkpoints_to_s3`, and `sync_s3_checkpoints_to_local`. For the purpose of this example, we are only uploading a checkpoint during training, by using `sync_local_checkpoints_to_s3`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have updated `entry_point`, `instance_count`, `instance_type` and `base_job_name`, run the following to create an estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "mpioptions = \"-verbose -x orte_base_help_aggregate=0 \"\n",
    "\n",
    "all_experiment_names = [exp.experiment_name for exp in Experiment.list()]\n",
    "\n",
    "#choose an experiment name (only need to create it once)\n",
    "experiment_name = \"SM-MP-DEMO\"\n",
    "\n",
    "# Load the experiment if it exists, otherwise create \n",
    "if experiment_name not in all_experiment_names:\n",
    "    customer_churn_experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=boto3.client(\"sagemaker\")\n",
    "    )\n",
    "else:\n",
    "    customer_churn_experiment = Experiment.load(\n",
    "        experiment_name=experiment_name, sagemaker_boto_client=boto3.client(\"sagemaker\")\n",
    "    )\n",
    "\n",
    "# Create a trial for the current run\n",
    "trial = Trial.create(\n",
    "        trial_name=\"SMD-MP-demo-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())),\n",
    "        experiment_name=customer_churn_experiment.experiment_name,\n",
    "        sagemaker_boto_client=boto3.client(\"sagemaker\"),\n",
    "    )\n",
    "\n",
    "\n",
    "smd_mp_estimator = PyTorch(\n",
    "          entry_point=\"pt_mnist.py\", # Pick your train script\n",
    "          source_dir='utils',\n",
    "          role=role,\n",
    "          instance_type='ml.p3.16xlarge',\n",
    "          sagemaker_session=sagemaker_session,\n",
    "          framework_version='1.6.0',\n",
    "          py_version='py36',\n",
    "          instance_count=1,\n",
    "          distribution={\n",
    "              \"smdistributed\": {\n",
    "                  \"modelparallel\": {\n",
    "                      \"enabled\":True,\n",
    "                      \"parameters\": {\n",
    "                          \"microbatches\": 4,\n",
    "                          \"placement_strategy\": \"spread\",\n",
    "                          \"pipeline\": \"interleaved\",\n",
    "                          \"optimize\": \"speed\",\n",
    "                          \"partitions\": 2,\n",
    "                          \"ddp\": True,\n",
    "                      }\n",
    "                  }\n",
    "              },\n",
    "              \"mpi\": {\n",
    "                    \"enabled\": True,\n",
    "                    \"processes_per_host\": 2, # Pick your processes_per_host\n",
    "                    \"custom_mpi_options\": mpioptions \n",
    "              },\n",
    "          },\n",
    "          base_job_name=\"SMD-MP-demo\",\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will use the estimator to launch the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: SMD-MP-demo-2021-01-18-05-08-17-225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-18 05:08:17 Starting - Starting the training job...\n",
      "2021-01-18 05:08:42 Starting - Launching requested ML instancesProfilerReport-1610946497: InProgress\n",
      ".........\n",
      "2021-01-18 05:10:16 Starting - Preparing the instances for training.........\n",
      "2021-01-18 05:11:44 Downloading - Downloading input data...\n",
      "2021-01-18 05:12:04 Training - Downloading the training image.................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-01-18 05:14:56,727 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-01-18 05:14:56,807 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-01-18 05:14:58,236 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-01-18 05:14:58,689 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-01-18 05:14:58,689 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-01-18 05:14:58,692 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-01-18 05:14:58,693 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1:2'] process_per_hosts: 2 num_processes: 2\u001b[0m\n",
      "\u001b[34m2021-01-18 05:14:58,697 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-01-18 05:14:58,775 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 2,\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-verbose -x orte_base_help_aggregate=0 \",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"mp_parameters\": {\n",
      "            \"microbatches\": 4,\n",
      "            \"placement_strategy\": \"spread\",\n",
      "            \"pipeline\": \"interleaved\",\n",
      "            \"optimize\": \"speed\",\n",
      "            \"partitions\": 2,\n",
      "            \"ddp\": true\n",
      "        }\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"SMD-MP-demo-2021-01-18-05-08-17-225\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-230755935769/SMD-MP-demo-2021-01-18-05-08-17-225/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"pt_mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"pt_mnist.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"mp_parameters\":{\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=pt_mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0 \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=pt_mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-230755935769/SMD-MP-demo-2021-01-18-05-08-17-225/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose -x orte_base_help_aggregate=0 \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":2},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"mp_parameters\":{\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"SMD-MP-demo-2021-01-18-05-08-17-225\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-230755935769/SMD-MP-demo-2021-01-18-05-08-17-225/source/sourcedir.tar.gz\",\"module_name\":\"pt_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"pt_mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--mp_parameters\",\"ddp=True,microbatches=4,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=spread\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"ddp\":true,\"microbatches\":4,\"optimize\":\"speed\",\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"spread\"}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:2 -np 2 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_HP_MP_PARAMETERS -x PYTHONPATH /opt/conda/bin/python -m mpi4py pt_mnist.py --mp_parameters ddp=True,microbatches=4,optimize=speed,partitions=2,pipeline=interleaved,placement_strategy=spread\n",
      "\n",
      "\n",
      " Data for JOB [41213,1] offset 0 Total slots allocated 2\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: algo-1#011Num slots: 2#011Max slots: 0#011Num procs: 2\n",
      " #011Process OMPI jobid: [41213,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41213,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:02.298: I smdistributed/modelparallel/torch/state_mod.py:114] [1] Finished initializing torch distributed process groups. mp_rank: 1, dp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:02.308: I smdistributed/modelparallel/torch/state_mod.py:114] [0] Finished initializing torch distributed process groups. mp_rank: 0, dp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Processing...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Done!\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.394 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:05.413 algo-1:33 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.510 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:05.510 algo-1:33 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.510 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:05.510 algo-1:33 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.510 algo-1:32 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:05.510 algo-1:33 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.511 algo-1:32 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:05.511 algo-1:33 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:05.511 algo-1:33 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.511 algo-1:32 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.622: I smdistributed/modelparallel/torch/worker.py:285] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.626 algo-1:32 INFO hook.py:550] name:net1.conv1.weight count_params:288\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.626 algo-1:32 INFO hook.py:550] name:net1.conv1.bias count_params:32\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.626 algo-1:32 INFO hook.py:550] name:net1.conv2.weight count_params:18432\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.627 algo-1:32 INFO hook.py:550] name:net1.conv2.bias count_params:64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.627 algo-1:32 INFO hook.py:550] name:net2.fc1.weight count_params:1179648\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.627 algo-1:32 INFO hook.py:550] name:net2.fc1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.627 algo-1:32 INFO hook.py:550] name:net2.fc2.weight count_params:1280\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.627 algo-1:32 INFO hook.py:550] name:net2.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.627 algo-1:32 INFO hook.py:552] Total Trainable Params: 1199882\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:05.627 algo-1:32 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-01-18 05:15:07 Training - Training image download completed. Training in progress.\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:13.064: I smdistributed/modelparallel/torch/module_partition.py:274] Partition assignments:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:13.064: I smdistributed/modelparallel/torch/module_partition.py:278] main: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:13.064: I smdistributed/modelparallel/torch/module_partition.py:278] main/module: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:13.064: I smdistributed/modelparallel/torch/module_partition.py:278] main/module/net1: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:13.064: I smdistributed/modelparallel/torch/module_partition.py:278] main/module/net2: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:13.067: I smdistributed/modelparallel/torch/model.py:324] Finished partitioning the model\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-01-18 05:15:13.067: I smdistributed/modelparallel/torch/model.py:280] Number of parameters on partition 0 are 4. 4 require grads\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:32 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.196.145<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:32 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:13.072: I smdistributed/modelparallel/torch/model.py:280] Number of parameters on partition 1 are 4. 4 require grads\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:33 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.196.145<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:33 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:32 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:33 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:32 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.196.145<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:33 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.196.145<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:NCCL version 2.4.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.4.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:234 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,ffffffff\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:233 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,ffffffff\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:234 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees enabled up to size -2\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:233 [1] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees enabled up to size -2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:32:234 [0] NCCL INFO comm 0x7fde1c003770 rank 0 nranks 1 cudaDev 0 nvmlDev 0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:33:233 [1] NCCL INFO comm 0x7f6810003770 rank 0 nranks 1 cudaDev 1 nvmlDev 1 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:13.173 algo-1:33 INFO hook.py:550] name:weight count_params:1179648\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:13.173 algo-1:33 INFO hook.py:550] name:bias count_params:128\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:13.173 algo-1:33 INFO hook.py:552] Total Trainable Params: 1179776\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-01-18 05:15:13.173 algo-1:33 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [0/60000 (0%)]#011Loss: 2.301630\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [640/60000 (1%)]#011Loss: 1.776750\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [1280/60000 (2%)]#011Loss: 0.451957\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [1920/60000 (3%)]#011Loss: 0.241554\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [2560/60000 (4%)]#011Loss: 0.405017\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [3200/60000 (5%)]#011Loss: 0.220497\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [3840/60000 (6%)]#011Loss: 0.146056\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [4480/60000 (7%)]#011Loss: 0.223679\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [5120/60000 (9%)]#011Loss: 0.306965\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [5760/60000 (10%)]#011Loss: 0.197951\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [6400/60000 (11%)]#011Loss: 0.286478\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [7040/60000 (12%)]#011Loss: 0.238190\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [7680/60000 (13%)]#011Loss: 0.108081\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [8320/60000 (14%)]#011Loss: 0.081834\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [8960/60000 (15%)]#011Loss: 0.141896\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [9600/60000 (16%)]#011Loss: 0.085953\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [10240/60000 (17%)]#011Loss: 0.176491\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [10880/60000 (18%)]#011Loss: 0.063994\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [11520/60000 (19%)]#011Loss: 0.533177\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [12160/60000 (20%)]#011Loss: 0.172299\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [12800/60000 (21%)]#011Loss: 0.102480\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [13440/60000 (22%)]#011Loss: 0.093992\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [14080/60000 (23%)]#011Loss: 0.066088\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [14720/60000 (25%)]#011Loss: 0.541470\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [15360/60000 (26%)]#011Loss: 0.115711\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [16000/60000 (27%)]#011Loss: 0.063112\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [16640/60000 (28%)]#011Loss: 0.181615\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [17280/60000 (29%)]#011Loss: 0.052158\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [17920/60000 (30%)]#011Loss: 0.135885\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [18560/60000 (31%)]#011Loss: 0.061471\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [19200/60000 (32%)]#011Loss: 0.094633\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [19840/60000 (33%)]#011Loss: 0.121780\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [20480/60000 (34%)]#011Loss: 0.025572\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [21120/60000 (35%)]#011Loss: 0.159990\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [21760/60000 (36%)]#011Loss: 0.011298\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [22400/60000 (37%)]#011Loss: 0.170092\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [23040/60000 (38%)]#011Loss: 0.068698\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [23680/60000 (39%)]#011Loss: 0.297330\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [24320/60000 (41%)]#011Loss: 0.003618\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [24960/60000 (42%)]#011Loss: 0.041367\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [25600/60000 (43%)]#011Loss: 0.109012\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [26240/60000 (44%)]#011Loss: 0.040308\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [26880/60000 (45%)]#011Loss: 0.144300\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [27520/60000 (46%)]#011Loss: 0.136579\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [28160/60000 (47%)]#011Loss: 0.021454\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [28800/60000 (48%)]#011Loss: 0.094466\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [29440/60000 (49%)]#011Loss: 0.095216\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [30080/60000 (50%)]#011Loss: 0.100609\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [30720/60000 (51%)]#011Loss: 0.043330\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [31360/60000 (52%)]#011Loss: 0.073588\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [32000/60000 (53%)]#011Loss: 0.175702\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [32640/60000 (54%)]#011Loss: 0.077425\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [33280/60000 (55%)]#011Loss: 0.106490\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [33920/60000 (57%)]#011Loss: 0.010031\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [34560/60000 (58%)]#011Loss: 0.017832\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [35200/60000 (59%)]#011Loss: 0.186794\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [35840/60000 (60%)]#011Loss: 0.123865\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [36480/60000 (61%)]#011Loss: 0.031458\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [37120/60000 (62%)]#011Loss: 0.045209\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [37760/60000 (63%)]#011Loss: 0.116096\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [38400/60000 (64%)]#011Loss: 0.058061\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [39040/60000 (65%)]#011Loss: 0.002185\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [39680/60000 (66%)]#011Loss: 0.076972\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [40320/60000 (67%)]#011Loss: 0.036836\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [40960/60000 (68%)]#011Loss: 0.094185\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [41600/60000 (69%)]#011Loss: 0.039437\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [42240/60000 (70%)]#011Loss: 0.042539\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [42880/60000 (71%)]#011Loss: 0.059788\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [43520/60000 (72%)]#011Loss: 0.134392\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [44160/60000 (74%)]#011Loss: 0.021467\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [44800/60000 (75%)]#011Loss: 0.158956\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [45440/60000 (76%)]#011Loss: 0.062919\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [46080/60000 (77%)]#011Loss: 0.149382\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [46720/60000 (78%)]#011Loss: 0.080274\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [47360/60000 (79%)]#011Loss: 0.189012\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [48000/60000 (80%)]#011Loss: 0.077417\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [48640/60000 (81%)]#011Loss: 0.003810\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [49280/60000 (82%)]#011Loss: 0.036290\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [49920/60000 (83%)]#011Loss: 0.106365\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [50560/60000 (84%)]#011Loss: 0.049764\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [51200/60000 (85%)]#011Loss: 0.122801\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [51840/60000 (86%)]#011Loss: 0.017895\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [52480/60000 (87%)]#011Loss: 0.026668\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [53120/60000 (88%)]#011Loss: 0.229757\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [53760/60000 (90%)]#011Loss: 0.141660\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [54400/60000 (91%)]#011Loss: 0.024510\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [55040/60000 (92%)]#011Loss: 0.004486\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [55680/60000 (93%)]#011Loss: 0.053406\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [56320/60000 (94%)]#011Loss: 0.079183\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [56960/60000 (95%)]#011Loss: 0.070476\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [57600/60000 (96%)]#011Loss: 0.108503\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [58240/60000 (97%)]#011Loss: 0.024963\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [58880/60000 (98%)]#011Loss: 0.008600\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Train Epoch: 1 [59520/60000 (99%)]#011Loss: 0.000265\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "smd_mp_estimator.fit(\n",
    "        experiment_config={\n",
    "            \"ExperimentName\": customer_churn_experiment.experiment_name,\n",
    "            \"TrialName\": trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
