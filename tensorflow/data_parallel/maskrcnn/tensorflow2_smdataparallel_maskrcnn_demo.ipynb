{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed data parallel MaskRCNN training with TensorFlow2 and SMDataParallel\n",
    "\n",
    "SMDataParallel is a new capability in Amazon SageMaker to train deep learning models faster and cheaper. SMDataParallel is a distributed data parallel training framework for TensorFlow, PyTorch, and MXNet.\n",
    "\n",
    "This notebook example shows how to use SMDataParallel with TensorFlow(version 2.3.1) on [Amazon SageMaker](https://aws.amazon.com/sagemaker/) to train a MaskRCNN model on [COCO 2017 dataset](https://cocodataset.org/#home) using [Amazon FSx for Lustre file-system](https://aws.amazon.com/fsx/lustre/) as data source.\n",
    "\n",
    "The outline of steps is as follows:\n",
    "\n",
    "1. Stage COCO 2017 dataset in [Amazon S3](https://aws.amazon.com/s3/)\n",
    "2. Create Amazon FSx Lustre file-system and import data into the file-system from S3\n",
    "3. Build Docker training image and push it to [Amazon ECR](https://aws.amazon.com/ecr/)\n",
    "4. Configure data input channels for SageMaker\n",
    "5. Configure hyper-prarameters\n",
    "6. Define training metrics\n",
    "7. Define training job, set distribution strategy to SMDataParallel and start training\n",
    "\n",
    "**NOTE:**  With large traning dataset, we recommend using (Amazon FSx)[https://aws.amazon.com/fsx/] as the input filesystem for the SageMaker training job. FSx file input to SageMaker significantly cuts down training start up time on SageMaker because it avoids downloading the training data each time you start the training job (as done with S3 input for SageMaker training job) and provides good data read throughput.\n",
    "\n",
    "\n",
    "**NOTE:** This example requires SageMaker Python SDK v2.X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Initialize the notebook instance. Get the aws region, sagemaker execution role.\n",
    "\n",
    "The IAM role arn used to give training and hosting access to your data. See the [Amazon SageMaker Roles](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with the appropriate full IAM role arn string(s). As described above, since we will be using FSx, please make sure to attach `FSx Access` permission to this IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (2.19.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.23.4.post0.tar.gz (396 kB)\n",
      "\u001b[K     |████████████████████████████████| 396 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: boto3>=1.16.32 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (1.16.37)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (1.19.4)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (3.14.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (3.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (20.7)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (1.19.37)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.16.32->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.16.32->sagemaker) (1.25.11)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-pasta->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-pasta->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-pasta->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sagemaker) (3.14.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-pasta->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3>=1.16.32->sagemaker) (1.19.37)\n",
      "Collecting smdebug_rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.23.4.post0-py2.py3-none-any.whl size=559728 sha256=64cbccea6853eb344235a19b009133e017b056512ca62f7568649c0dd2df3078\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/f4/bf/4d/b0f85f925dd2e323e2e159e33204ae7340fd55bf4745797a3d\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.0\n",
      "    Uninstalling smdebug-rulesconfig-1.0.0:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.19.0\n",
      "    Uninstalling sagemaker-2.19.0:\n",
      "      Successfully uninstalled sagemaker-2.19.0\n",
      "Successfully installed sagemaker-2.23.4.post0 smdebug-rulesconfig-1.0.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "CPU times: user 115 ms, sys: 36.6 ms, total: 152 ms\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! python3 -m pip install --upgrade sagemaker\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role:arn:aws:iam::230755935769:role/SageMakerExecutionRoleMLOps\n",
      "AWS account:230755935769\n",
      "AWS region:us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = get_execution_role() # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f'SageMaker Execution Role:{role}')\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "print(f'AWS account:{account}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f'AWS region:{region}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare SageMaker Training Images\n",
    "\n",
    "1. SageMaker by default use the latest [Amazon Deep Learning Container Images (DLC)](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) TensorFlow training image. In this step, we use it as a base image and install additional dependencies required for training MaskRCNN model.\n",
    "2. In the Github repository https://github.com/HerringForks/DeepLearningExamples.git we have made TensorFlow-SMDataParallel MaskRCNN training script available for your use. We will be installing the same on the training image.\n",
    "\n",
    "### Build and Push Docker Image to ECR\n",
    "\n",
    "Run the below command build the docker image and push it to ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"tf2-mask-rcnn-smdataparallel-sagemaker\"  # Example: tf2-mask-rcnn-smdataparallel-sagemaker\n",
    "tag = \"latest\"   # Example: latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mARG\u001b[39;49;00m region\r\n",
      "\r\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mRUN\u001b[39;49;00m \tpip --no-cache-dir --no-cache install \u001b[33m\\\u001b[39;49;00m\r\n",
      "        Cython \u001b[33m\\\u001b[39;49;00m\r\n",
      "        matplotlib \u001b[33m\\\u001b[39;49;00m\r\n",
      "        opencv-python-headless \u001b[33m\\\u001b[39;49;00m\r\n",
      "        mpi4py \u001b[33m\\\u001b[39;49;00m\r\n",
      "        Pillow \u001b[33m\\\u001b[39;49;00m\r\n",
      "        pytest \u001b[33m\\\u001b[39;49;00m\r\n",
      "        pyyaml\r\n",
      "\r\n",
      "\u001b[34mRUN\u001b[39;49;00m \t\u001b[36mcd\u001b[39;49;00m /root && \u001b[33m\\\u001b[39;49;00m\r\n",
      "\tgit clone https://github.com/pybind/pybind11 && \u001b[33m\\\u001b[39;49;00m\r\n",
      "\t\u001b[36mcd\u001b[39;49;00m pybind11 && \u001b[33m\\\u001b[39;49;00m\r\n",
      "\tcmake . && \u001b[33m\\\u001b[39;49;00m\r\n",
      "\tmake -j96 install && \u001b[33m\\\u001b[39;49;00m\r\n",
      "\tpip install .\r\n",
      "\r\n",
      "\u001b[34mRUN\u001b[39;49;00m \tpip --no-cache-dir --no-cache install \u001b[33m\\\u001b[39;49;00m\r\n",
      "    \t\u001b[33m'git+https://github.com/NVIDIA/cocoapi#egg=pycocotools&subdirectory=PythonAPI'\u001b[39;49;00m && \u001b[33m\\\u001b[39;49;00m\r\n",
      "\tpip --no-cache-dir --no-cache install \u001b[33m\\\u001b[39;49;00m\r\n",
      "    \t\u001b[33m'git+https://github.com/NVIDIA/dllogger'\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env bash\u001b[39;49;00m\r\n",
      "\u001b[37m# This script shows how to build the Docker image and push it to ECR to be ready for use\u001b[39;49;00m\r\n",
      "\u001b[37m# by SageMaker.\u001b[39;49;00m\r\n",
      "\u001b[37m# The argument to this script is the image name. This will be used as the image on the local\u001b[39;49;00m\r\n",
      "\u001b[37m# machine and combined with the account and region to form the repository name for ECR.\u001b[39;49;00m\r\n",
      "\u001b[37m# set region\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[31mDIR\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[34m$(\u001b[39;49;00m \u001b[36mcd\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[34m$(\u001b[39;49;00m dirname \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mBASH_SOURCE\u001b[39;49;00m[0]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m && \u001b[36mpwd\u001b[39;49;00m \u001b[34m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[33m\"\u001b[39;49;00m\u001b[31m$#\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m -eq \u001b[34m3\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\r\n",
      "    \u001b[31mregion\u001b[39;49;00m=\u001b[31m$1\u001b[39;49;00m\r\n",
      "    \u001b[31mimage\u001b[39;49;00m=\u001b[31m$2\u001b[39;49;00m\r\n",
      "    \u001b[31mtag\u001b[39;49;00m=\u001b[31m$3\u001b[39;49;00m\r\n",
      "\u001b[34melse\u001b[39;49;00m\r\n",
      "    \u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33musage: \u001b[39;49;00m\u001b[31m$0\u001b[39;49;00m\u001b[33m <aws-region> \u001b[39;49;00m\u001b[31m$1\u001b[39;49;00m\u001b[33m <image-repo> \u001b[39;49;00m\u001b[31m$2\u001b[39;49;00m\u001b[33m <image-tag>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[36mexit\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\r\n",
      "\u001b[34mfi\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Get the account number associated with the current IAM credentials\u001b[39;49;00m\r\n",
      "\u001b[31maccount\u001b[39;49;00m=\u001b[34m$(\u001b[39;49;00maws sts get-caller-identity --query Account --output text\u001b[34m)\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -ne \u001b[34m0\u001b[39;49;00m ]\r\n",
      "\u001b[34mthen\u001b[39;49;00m\r\n",
      "    \u001b[36mexit\u001b[39;49;00m \u001b[34m255\u001b[39;49;00m\r\n",
      "\u001b[34mfi\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[31mfullname\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31maccount\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.dkr.ecr.\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.amazonaws.com/\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mtag\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# If the repository doesn't exist in ECR, create it.\u001b[39;49;00m\r\n",
      "aws ecr describe-repositories --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --repository-names \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m > /dev/null \u001b[34m2\u001b[39;49;00m>&\u001b[34m1\u001b[39;49;00m\r\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -ne \u001b[34m0\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\r\n",
      "    \u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcreating ECR repository : \u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    aws ecr create-repository --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --repository-name \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m > /dev/null\r\n",
      "\u001b[34mfi\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34m$(\u001b[39;49;00maws ecr get-login --no-include-email --region us-west-2  --registry-ids \u001b[34m763104351884\u001b[39;49;00m\u001b[34m)\u001b[39;49;00m\r\n",
      "docker build \u001b[33m${\u001b[39;49;00m\u001b[31mDIR\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m/ -t \u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m -f \u001b[33m${\u001b[39;49;00m\u001b[31mDIR\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m/Dockerfile  --build-arg \u001b[31mregion\u001b[39;49;00m=\u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\r\n",
      "docker tag \u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m \u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Get the login command from ECR and execute it directly\u001b[39;49;00m\r\n",
      "\u001b[34m$(\u001b[39;49;00maws ecr get-login --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --no-include-email\u001b[34m)\u001b[39;49;00m\r\n",
      "docker push \u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\r\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -eq \u001b[34m0\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\r\n",
      "\t\u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mAmazon ECR URI: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\u001b[34melse\u001b[39;49;00m\r\n",
      "\t\u001b[36mecho\u001b[39;49;00m \u001b[33m\"Error: Image build and push failed\"\u001b[39;49;00m\r\n",
      "\t\u001b[36mexit\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\r\n",
      "\u001b[34mfi\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  285.2kB\n",
      "Step 1/5 : ARG region\n",
      "Step 2/5 : FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04\n",
      "2.3.1-gpu-py37-cu110-ubuntu18.04: Pulling from tensorflow-training\n",
      "\n",
      "\u001b[1B57c49d0f: Pulling fs layer \n",
      "\u001b[1B40447d26: Pulling fs layer \n",
      "\u001b[1B2f862619: Pulling fs layer \n",
      "\u001b[1B278deddf: Pulling fs layer \n",
      "\u001b[1B80049843: Pulling fs layer \n",
      "\u001b[1B556b2329: Pulling fs layer \n",
      "\u001b[1B9f1ec0f3: Pulling fs layer \n",
      "\u001b[3B556b2329: Waiting fs layer \n",
      "\u001b[3B9f1ec0f3: Waiting fs layer \n",
      "\u001b[3B3d9de4de: Waiting fs layer \n",
      "\u001b[1B3ad2a50a: Pulling fs layer \n",
      "\u001b[4Bc51c9781: Waiting fs layer \n",
      "\u001b[1Bbadfd864: Pulling fs layer \n",
      "\u001b[4B3ad2a50a: Waiting fs layer \n",
      "\u001b[1B035444b2: Pulling fs layer \n",
      "\u001b[3Ba1a16d74: Waiting fs layer \n",
      "\u001b[3B035444b2: Waiting fs layer \n",
      "\u001b[3Ba6b13424: Waiting fs layer \n",
      "\u001b[8Be11c6a81: Waiting fs layer \n",
      "\u001b[1B6f03c368: Pulling fs layer \n",
      "\u001b[3B4768ea01: Waiting fs layer \n",
      "\u001b[6Bc75ad910: Waiting fs layer \n",
      "\u001b[2B4723019a: Waiting fs layer \n",
      "\u001b[1B6d5b5f9b: Pulling fs layer \n",
      "\u001b[1B8eac5355: Pulling fs layer \n",
      "\u001b[1B19388010: Pulling fs layer \n",
      "\u001b[1B36cab2a0: Pulling fs layer \n",
      "\u001b[4B8eac5355: Waiting fs layer \n",
      "\u001b[1Bdf0c39eb: Pulling fs layer \n",
      "\u001b[1Bf46901b2: Pulling fs layer \n",
      "\u001b[1B245b8b4b: Pulling fs layer \n",
      "\u001b[5B56e443dd: Waiting fs layer \n",
      "\u001b[1Bd35df602: Pulling fs layer \n",
      "\u001b[3B1bde60b9: Waiting fs layer \n",
      "\u001b[1B1ee0bab2: Pulling fs layer \n",
      "\u001b[3B86b201ef: Waiting fs layer \n",
      "\u001b[5Bd35df602: Waiting fs layer \n",
      "\u001b[1B2737200c: Pulling fs layer \n",
      "\u001b[5B1ee0bab2: Waiting fs layer \n",
      "\u001b[5Bb8ffa246: Waiting fs layer \n",
      "\u001b[1B067e0696: Pulling fs layer \n",
      "\u001b[4B1ceac894: Waiting fs layer \n",
      "\u001b[1Be8f270b0: Pulling fs layer \n",
      "\u001b[4B067e0696: Waiting fs layer \n",
      "\u001b[3Be8f270b0: Waiting fs layer \n",
      "\u001b[3Bbc65a746: Waiting fs layer \n",
      "\u001b[1Ba9ccfd39: Pulling fs layer \n",
      "\u001b[1B83295919: Pulling fs layer \n",
      "\u001b[2B83295919: Waiting fs layer \n",
      "\u001b[1Bb7091d4f: Pulling fs layer \n",
      "\u001b[1B1966e0ad: Pulling fs layer \n",
      "\u001b[2B1966e0ad: Waiting fs layer \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[45B51c9781: Extracting  1.094GB/2.914GBB[49A\u001b[2K\u001b[49A\u001b[2K\u001b[53A\u001b[2K\u001b[47A\u001b[2K\u001b[46A\u001b[2K\u001b[53A\u001b[2K\u001b[45A\u001b[2K\u001b[53A\u001b[2K\u001b[47A\u001b[2K\u001b[53A\u001b[2K\u001b[45A\u001b[2K\u001b[53A\u001b[2K\u001b[44A\u001b[2K\u001b[53A\u001b[2K\u001b[44A\u001b[2K\u001b[53A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[53A\u001b[2K\u001b[45A\u001b[2K\u001b[52A\u001b[2K\u001b[51A\u001b[2K\u001b[47A\u001b[2KDownloading  47.25MB/345.8MB\u001b[50A\u001b[2K\u001b[47A\u001b[2K\u001b[50A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[49A\u001b[2K\u001b[47A\u001b[2K\u001b[49A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[47A\u001b[2K\u001b[44A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[47A\u001b[2K\u001b[43A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[41A\u001b[2K\u001b[40A\u001b[2K\u001b[39A\u001b[2K\u001b[38A\u001b[2K\u001b[37A\u001b[2K\u001b[47A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[45A\u001b[2K\u001b[36A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[35A\u001b[2K\u001b[45A\u001b[2K\u001b[36A\u001b[2K\u001b[45A\u001b[2K\u001b[33A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[45A\u001b[2K\u001b[34A\u001b[2K\u001b[45A\u001b[2K\u001b[47A\u001b[2K\u001b[30A\u001b[2K\u001b[47A\u001b[2K\u001b[34A\u001b[2K\u001b[45A\u001b[2K\u001b[29A\u001b[2K\u001b[28A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[27A\u001b[2K\u001b[45A\u001b[2K\u001b[27A\u001b[2K\u001b[45A\u001b[2K\u001b[26A\u001b[2K\u001b[45A\u001b[2K\u001b[26A\u001b[2K\u001b[47A\u001b[2K\u001b[25A\u001b[2K\u001b[47A\u001b[2K\u001b[30A\u001b[2K\u001b[24A\u001b[2K\u001b[30A\u001b[2K\u001b[47A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[30A\u001b[2K\u001b[47A\u001b[2K\u001b[20A\u001b[2K\u001b[45A\u001b[2K\u001b[19A\u001b[2K\u001b[47A\u001b[2K\u001b[18A\u001b[2K\u001b[46A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[45A\u001b[2K\u001b[14A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[13A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[13A\u001b[2K\u001b[45A\u001b[2K\u001b[12A\u001b[2K\u001b[45A\u001b[2K\u001b[12A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[12A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[11A\u001b[2K\u001b[30A\u001b[2K\u001b[11A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[30A\u001b[2K\u001b[11A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[30A\u001b[2K\u001b[11A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[30A\u001b[2K\u001b[11A\u001b[2K\u001b[30A\u001b[2K\u001b[11A\u001b[2K\u001b[45A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[9A\u001b[2K\u001b[30A\u001b[2K\u001b[9A\u001b[2K\u001b[45A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[45A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[30A\u001b[2K\u001b[6A\u001b[2K\u001b[45A\u001b[2K\u001b[5A\u001b[2K\u001b[30A\u001b[2K\u001b[4A\u001b[2K\u001b[45A\u001b[2K\u001b[4A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[4A\u001b[2K\u001b[45A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[30A\u001b[2K\u001b[1A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[1Be2aa5793: Pull complete .33kB/38.33kB\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[45A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[44A\u001b[2K\u001b[43A\u001b[2K\u001b[43A\u001b[2K\u001b[43A\u001b[2K\u001b[43A\u001b[2K\u001b[42A\u001b[2K\u001b[41A\u001b[2K\u001b[40A\u001b[2K\u001b[40A\u001b[2K\u001b[39A\u001b[2K\u001b[38A\u001b[2K\u001b[38A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[37A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[34A\u001b[2K\u001b[33A\u001b[2K\u001b[33A\u001b[2K\u001b[32A\u001b[2K\u001b[32A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2KExtracting  507.5MB/591.3MB\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[30A\u001b[2K\u001b[29A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[27A\u001b[2K\u001b[27A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[26A\u001b[2K\u001b[25A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KDigest: sha256:9266326cd7d9ab373d3a17ee703ca67770dad1d8341e025e376003f1a2d432cd\n",
      "Status: Downloaded newer image for 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04\n",
      " ---> 32790263d5eb\n",
      "Step 3/5 : RUN \tpip --no-cache-dir --no-cache install         Cython         matplotlib         opencv-python-headless         mpi4py         Pillow         pytest         pyyaml\n",
      " ---> Running in 6d6798c3aa47\n",
      "Requirement already satisfied: mpi4py in /usr/local/lib/python3.7/site-packages (3.0.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/site-packages (7.2.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (5.3.1)\n",
      "Collecting Cython\n",
      "  Downloading Cython-0.29.21-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.3-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.18.5)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.1.48-cp37-cp37m-manylinux2014_x86_64.whl (37.6 MB)\n",
      "Collecting pytest\n",
      "  Downloading pytest-6.2.1-py3-none-any.whl (279 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/site-packages (from pytest) (20.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/site-packages (from pytest) (3.3.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/site-packages (from pytest) (20.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.7.4.3)\n",
      "Collecting pluggy<1.0.0a1,>=0.12\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: toml, py, pluggy, kiwisolver, iniconfig, cycler, pytest, opencv-python-headless, matplotlib, Cython\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Cython-0.29.21 cycler-0.10.0 iniconfig-1.1.1 kiwisolver-1.3.1 matplotlib-3.3.3 opencv-python-headless-4.5.1.48 pluggy-0.13.1 py-1.10.0 pytest-6.2.1 toml-0.10.2\n",
      "Removing intermediate container 6d6798c3aa47\n",
      " ---> c0b53859ba55\n",
      "Step 4/5 : RUN \tcd /root && \tgit clone https://github.com/pybind/pybind11 && \tcd pybind11 && \tcmake . && \tmake -j96 install && \tpip install .\n",
      " ---> Running in 8c8d9c172a66\n",
      "\u001b[91mCloning into 'pybind11'...\n",
      "\u001b[0m-- The CXX compiler identification is GNU 7.5.0\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- pybind11 v2.6.2 dev1\n",
      "\u001b[91mCMake Warning (dev) at CMakeLists.txt:57 (message):\n",
      "  You are building in-place.  If that is not what you intended to do, you can\n",
      "  clean the source directory with:\n",
      "\n",
      "  rm -r CMakeCache.txt CMakeFiles/ cmake_uninstall.cmake pybind11Config.cmake\n",
      "  pybind11ConfigVersion.cmake tests/CMakeFiles/\n",
      "\n",
      "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\n",
      "\u001b[0m-- CMake 3.18.2\n",
      "-- Found PythonInterp: /usr/local/bin/python3.7 (found version \"3.7.7\") \n",
      "-- Found PythonLibs: /usr/local/lib/libpython3.7m.so\n",
      "-- PYTHON 3.7.7\n",
      "-- Performing Test HAS_FLTO\n",
      "-- Performing Test HAS_FLTO - Success\n",
      "-- pybind11::lto enabled\n",
      "-- pybind11::thin_lto enabled\n",
      "-- Setting tests build type to MinSizeRel as none was specified\n",
      "-- Building tests WITHOUT Eigen, use -DDOWNLOAD_EIGEN on CMake 3.11+ to download\n",
      "-- Found Boost: /usr/lib/cmake/Boost-1.73.0/BoostConfig.cmake (found suitable version \"1.73.0\", minimum required is \"1.56\")  \n",
      "-- Found pytest 6.2.1\n",
      "-- Catch not detected. Interpreter tests will be skipped. Install Catch headers manually or use `cmake -DDOWNLOAD_CATCH=ON` to fetch them automatically.\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /root/pybind11\n",
      "Scanning dependencies of target cross_module_gil_utils\n",
      "Scanning dependencies of target pybind11_cross_module_tests\n",
      "[  2%] Building CXX object tests/CMakeFiles/cross_module_gil_utils.dir/cross_module_gil_utils.cpp.o\n",
      "[  4%] Building CXX object tests/CMakeFiles/pybind11_cross_module_tests.dir/pybind11_cross_module_tests.cpp.o\n",
      "Scanning dependencies of target pybind11_tests\n",
      "[  7%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/pybind11_tests.cpp.o\n",
      "[  9%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_async.cpp.o\n",
      "[ 11%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_buffers.cpp.o\n",
      "[ 14%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_builtin_casters.cpp.o\n",
      "[ 16%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_chrono.cpp.o\n",
      "[ 19%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_call_policies.cpp.o\n",
      "[ 21%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_class.cpp.o\n",
      "[ 23%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_callbacks.cpp.o\n",
      "[ 26%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_constants_and_functions.cpp.o\n",
      "[ 28%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_eval.cpp.o\n",
      "[ 30%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_gil_scoped.cpp.o\n",
      "[ 33%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_iostream.cpp.o\n",
      "[ 35%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_copy_move.cpp.o\n",
      "[ 38%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_custom_type_casters.cpp.o\n",
      "[ 40%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_factory_constructors.cpp.o\n",
      "[ 45%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_opaque_types.cpp.o\n",
      "[ 45%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_stl_binders.cpp.o\n",
      "[ 47%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_local_bindings.cpp.o\n",
      "[ 50%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_tagbased_polymorphic.cpp.o\n",
      "[ 52%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_pytypes.cpp.o\n",
      "[ 57%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_kwargs_and_defaults.cpp.o\n",
      "[ 59%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_methods_and_attributes.cpp.o\n",
      "[ 61%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_exceptions.cpp.o\n",
      "[ 64%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_enum.cpp.o\n",
      "[ 66%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_numpy_vectorize.cpp.o\n",
      "[ 54%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_multiple_inheritance.cpp.o\n",
      "[ 69%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_sequences_and_iterators.cpp.o\n",
      "[ 71%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_numpy_dtypes.cpp.o\n",
      "[ 73%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_smart_ptr.cpp.o\n",
      "[ 76%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_virtual_functions.cpp.o\n",
      "[ 78%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_operator_overloading.cpp.o\n",
      "[ 80%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_numpy_array.cpp.o\n",
      "[ 83%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_union.cpp.o\n",
      "[ 85%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_modules.cpp.o\n",
      "[ 88%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_docstring_options.cpp.o\n",
      "[ 90%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_pickling.cpp.o\n",
      "[ 92%] Building CXX object tests/CMakeFiles/pybind11_tests.dir/test_stl.cpp.o\n",
      "[ 95%] Linking CXX shared module cross_module_gil_utils.cpython-37m-x86_64-linux-gnu.so\n",
      "[ 95%] Built target cross_module_gil_utils\n",
      "[ 97%] Linking CXX shared module pybind11_cross_module_tests.cpython-37m-x86_64-linux-gnu.so\n",
      "[ 97%] Built target pybind11_cross_module_tests\n",
      "[100%] Linking CXX shared module pybind11_tests.cpython-37m-x86_64-linux-gnu.so\n",
      "------ pybind11_tests.cpython-37m-x86_64-linux-gnu.so file size: 2151032\n",
      "[100%] Built target pybind11_tests\n",
      "Install the project...\n",
      "-- Install configuration: \"MinSizeRel\"\n",
      "-- Installing: /usr/local/include/pybind11\n",
      "-- Installing: /usr/local/include/pybind11/buffer_info.h\n",
      "-- Installing: /usr/local/include/pybind11/stl.h\n",
      "-- Installing: /usr/local/include/pybind11/attr.h\n",
      "-- Installing: /usr/local/include/pybind11/eval.h\n",
      "-- Installing: /usr/local/include/pybind11/pybind11.h\n",
      "-- Installing: /usr/local/include/pybind11/common.h\n",
      "-- Installing: /usr/local/include/pybind11/detail\n",
      "-- Installing: /usr/local/include/pybind11/detail/internals.h\n",
      "-- Installing: /usr/local/include/pybind11/detail/common.h\n",
      "-- Installing: /usr/local/include/pybind11/detail/descr.h\n",
      "-- Installing: /usr/local/include/pybind11/detail/typeid.h\n",
      "-- Installing: /usr/local/include/pybind11/detail/class.h\n",
      "-- Installing: /usr/local/include/pybind11/detail/init.h\n",
      "-- Installing: /usr/local/include/pybind11/functional.h\n",
      "-- Installing: /usr/local/include/pybind11/pytypes.h\n",
      "-- Installing: /usr/local/include/pybind11/operators.h\n",
      "-- Installing: /usr/local/include/pybind11/eigen.h\n",
      "-- Installing: /usr/local/include/pybind11/stl_bind.h\n",
      "-- Installing: /usr/local/include/pybind11/complex.h\n",
      "-- Installing: /usr/local/include/pybind11/numpy.h\n",
      "-- Installing: /usr/local/include/pybind11/embed.h\n",
      "-- Installing: /usr/local/include/pybind11/iostream.h\n",
      "-- Installing: /usr/local/include/pybind11/cast.h\n",
      "-- Installing: /usr/local/include/pybind11/chrono.h\n",
      "-- Installing: /usr/local/include/pybind11/options.h\n",
      "-- Installing: /usr/local/share/cmake/pybind11/pybind11Config.cmake\n",
      "-- Installing: /usr/local/share/cmake/pybind11/pybind11ConfigVersion.cmake\n",
      "-- Installing: /usr/local/share/cmake/pybind11/FindPythonLibsNew.cmake\n",
      "-- Installing: /usr/local/share/cmake/pybind11/pybind11Common.cmake\n",
      "-- Installing: /usr/local/share/cmake/pybind11/pybind11Tools.cmake\n",
      "-- Installing: /usr/local/share/cmake/pybind11/pybind11NewTools.cmake\n",
      "-- Installing: /usr/local/share/cmake/pybind11/pybind11Targets.cmake\n",
      "Processing /root/pybind11\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Building wheels for collected packages: pybind11\n",
      "  Building wheel for pybind11 (PEP 517): started\n",
      "  Building wheel for pybind11 (PEP 517): finished with status 'done'\n",
      "  Created wheel for pybind11: filename=pybind11-2.6.2.dev1-py2.py3-none-any.whl size=190538 sha256=03145db1eefa7dc13e0fd9fa6bbd3e325c687b3394b9a194426968dde5d52a18\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pmnt69dg/wheels/2a/e8/f8/e5daba934e808130ebb389b2832ca810f17864b69c253e5058\n",
      "Successfully built pybind11\n",
      "Installing collected packages: pybind11\n",
      "  Attempting uninstall: pybind11\n",
      "    Found existing installation: pybind11 2.6.1\n",
      "    Uninstalling pybind11-2.6.1:\n",
      "      Successfully uninstalled pybind11-2.6.1\n",
      "Successfully installed pybind11-2.6.2.dev1\n",
      "Removing intermediate container 8c8d9c172a66\n",
      " ---> 169ca864d021\n",
      "Step 5/5 : RUN \tpip --no-cache-dir --no-cache install     \t'git+https://github.com/NVIDIA/cocoapi#egg=pycocotools&subdirectory=PythonAPI' && \tpip --no-cache-dir --no-cache install     \t'git+https://github.com/NVIDIA/dllogger'\n",
      " ---> Running in be4bf5a22e5e\n",
      "Collecting pycocotools\n",
      "  Cloning https://github.com/NVIDIA/cocoapi to /tmp/pip-install-ulidw8nk/pycocotools_478ca8d924f344bc94ec470c65b701ad\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/site-packages (from pycocotools) (51.0.0)\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/site-packages (from pycocotools) (0.29.21)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/site-packages (from pycocotools) (3.3.3)\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/site-packages (from pycocotools) (2.6.2.dev1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.18.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py): started\n",
      "  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0+nv0.4.0-cp37-cp37m-linux_x86_64.whl size=1296327 sha256=78ae7b16d9d747b719f0516194dc6d570745740e4c1f441aa4bda4f267f91704\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-10josbjk/wheels/f2/c2/58/b2e24e79810b9ec026b17a2df3e9d063b75e40a850de3e3828\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0+nv0.4.0\n",
      "Collecting git+https://github.com/NVIDIA/dllogger\n",
      "  Cloning https://github.com/NVIDIA/dllogger to /tmp/pip-req-build-dy94w610\n",
      "Building wheels for collected packages: DLLogger\n",
      "  Building wheel for DLLogger (setup.py): started\n",
      "  Building wheel for DLLogger (setup.py): finished with status 'done'\n",
      "  Created wheel for DLLogger: filename=DLLogger-0.1.0-py3-none-any.whl size=5616 sha256=6c65d5341ec59f493dfaa06642edb48bb382de10586e3e160befd8bfb5876122\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ca06525i/wheels/f6/0b/e6/d9566189ca2a94642aa7060bbf15c51ec8c464053945b1358b\n",
      "Successfully built DLLogger\n",
      "Installing collected packages: DLLogger\n",
      "Successfully installed DLLogger-0.1.0\n",
      "Removing intermediate container be4bf5a22e5e\n",
      " ---> 526248d0260c\n",
      "Successfully built 526248d0260c\n",
      "Successfully tagged tf2-mask-rcnn-smdataparallel-sagemaker:latest\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "The push refers to repository [230755935769.dkr.ecr.us-west-2.amazonaws.com/tf2-mask-rcnn-smdataparallel-sagemaker]\n",
      "\n",
      "\u001b[1B4377c2e0: Preparing \n",
      "\u001b[1B0620a50a: Preparing \n",
      "\u001b[1Bcaa6cb3d: Preparing \n",
      "\u001b[1B4b583b39: Preparing \n",
      "\u001b[1B218999f2: Preparing \n",
      "\u001b[1Bd8281563: Preparing \n",
      "\u001b[1B86e17581: Preparing \n",
      "\u001b[1Bdb0aff6e: Preparing \n",
      "\u001b[1Bad825469: Preparing \n",
      "\u001b[1Bafc213b4: Preparing \n",
      "\u001b[1B8a652982: Preparing \n",
      "\u001b[1B823d14f0: Preparing \n",
      "\u001b[1Bcee1417c: Preparing \n",
      "\u001b[1Bad0e16c4: Preparing \n",
      "\u001b[1Bce8b5eaf: Preparing \n",
      "\u001b[1Bd74b1ac5: Preparing \n",
      "\u001b[1B2e14999b: Preparing \n",
      "\u001b[12B6e17581: Waiting g \n",
      "\u001b[1B9c3a705d: Preparing \n",
      "\u001b[10Ba652982: Waiting g \n",
      "\u001b[12Bfc213b4: Waiting g \n",
      "\u001b[8Bce8b5eaf: Waiting g \n",
      "\u001b[7B2e14999b: Waiting g \n",
      "\u001b[7B163589af: Waiting g \n",
      "\u001b[6Bbc741e99: Waiting g \n",
      "\u001b[8B9c3a705d: Waiting g \n",
      "\u001b[1B6ce4ecae: Preparing \n",
      "\u001b[6B8a8ff5aa: Waiting g \n",
      "\u001b[6B1bfe0d0f: Waiting g \n",
      "\u001b[25B8281563: Waiting g \n",
      "\u001b[1Bd1b73987: Preparing \n",
      "\u001b[8Beae1c3d4: Waiting g \n",
      "\u001b[1Ba9807c20: Preparing \n",
      "\u001b[9B2c2a7772: Waiting g \n",
      "\u001b[1B3b8ee1b6: Preparing \n",
      "\u001b[10Bce4ecae: Waiting g \n",
      "\u001b[1Bf286b156: Preparing \n",
      "\u001b[11B7a42883: Waiting g \n",
      "\u001b[1B72886db9: Preparing \n",
      "\u001b[12B2d09be1: Waiting g \n",
      "\u001b[1B60409914: Preparing \n",
      "\u001b[13Bc2ba6c4: Waiting g \n",
      "\u001b[1Bbd249da9: Preparing \n",
      "\u001b[14B1b73987: Waiting g \n",
      "\u001b[1B7a382dc5: Preparing \n",
      "\u001b[1B199171c5: Preparing \n",
      "\u001b[1B51e616d4: Preparing \n",
      "\u001b[17B6e63e8c: Waiting g \n",
      "\u001b[1Bf414295e: Preparing \n",
      "\u001b[1Bc59a8b8c: Preparing \n",
      "\u001b[19B9807c20: Waiting g \n",
      "\u001b[1B30bcc944: Preparing \n",
      "\u001b[1Be116c0c0: Preparing \n",
      "\u001b[21B727dfa8: Waiting g \n",
      "\u001b[1Bdf553184: Preparing \n",
      "\u001b[54Baa6cb3d: Pushed   149.7MB/148.3MB1A\u001b[2K\u001b[49A\u001b[2K\u001b[46A\u001b[2K\u001b[45A\u001b[2K\u001b[43A\u001b[2K\u001b[54A\u001b[2K\u001b[56A\u001b[2K\u001b[55A\u001b[2K\u001b[34A\u001b[2K\u001b[32A\u001b[2K\u001b[30A\u001b[2K\u001b[27A\u001b[2K\u001b[26A\u001b[2K\u001b[22A\u001b[2K\u001b[55A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[9A\u001b[2K\u001b[6A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[55A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[55A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[55A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2K\u001b[54A\u001b[2Klatest: digest: sha256:13c481655eb7c1300bf20ddbe042ee341891739b6ed0dc81abc8d6da7c5cba71 size: 12083\n",
      "Amazon ECR URI: 230755935769.dkr.ecr.us-west-2.amazonaws.com/tf2-mask-rcnn-smdataparallel-sagemaker:latest\n",
      "CPU times: user 4.15 s, sys: 1.03 s, total: 5.17 s\n",
      "Wall time: 5min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! chmod +x build_and_push.sh; bash build_and_push.sh {region} {image} {tag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing FSx Input for SageMaker\n",
    "\n",
    "1. Download and prepare your training dataset on S3.\n",
    "2. Follow the steps listed here to create a FSx linked with your S3 bucket with training data - https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-fs-linked-data-repo.html. Make sure to add an endpoint to your VPC allowing S3 access.\n",
    "3. Follow the steps listed here to configure your SageMaker training job to use FSx https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/\n",
    "\n",
    "### Important Caveats\n",
    "\n",
    "1. You need use the same `subnet` and `vpc` and `security group` used with FSx when launching the SageMaker notebook instance. The same configurations will be used by your SageMaker training job.\n",
    "2. Make sure you set appropriate inbound/output rules in the `security group`. Specically, opening up these ports is necessary for SageMaker to access the FSx filesystem in the training job. https://docs.aws.amazon.com/fsx/latest/LustreGuide/limit-access-security-groups.html\n",
    "3. Make sure `SageMaker IAM Role` used to launch this SageMaker training job has access to `AmazonFSx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker TensorFlow Estimator function options\n",
    "\n",
    "In the following code block, you can update the estimator function to use a different instance type, instance count, and distrubtion strategy. You're also passing in the training script you reviewed in the previous cell.\n",
    "\n",
    "**Instance types**\n",
    "\n",
    "SMDataParallel supports model training on SageMaker with the following instance types only:\n",
    "1. ml.p3.16xlarge\n",
    "1. ml.p3dn.24xlarge [Recommended]\n",
    "1. ml.p4d.24xlarge [Recommended]\n",
    "\n",
    "**Instance count**\n",
    "\n",
    "To get the best performance and the most out of SMDataParallel, you should use at least 2 instances, but you can also use 1 for testing this example.\n",
    "\n",
    "**Distribution strategy**\n",
    "\n",
    "Note that to use DDP mode, you update the the `distribution` strategy, and set it to use `smdistributed dataparallel`.\n",
    "\n",
    "### Training script\n",
    "\n",
    "In the Github repository https://github.com/HerringForks/DeepLearningExamples.git we have made reference TensorFlow-SMDataParallel MaskRCNN training script available for your use. Clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepLearningExamples'...\n",
      "remote: Enumerating objects: 23025, done.\u001b[K\n",
      "remote: Total 23025 (delta 0), reused 0 (delta 0), pack-reused 23025\u001b[K\n",
      "Receiving objects: 100% (23025/23025), 57.78 MiB | 17.92 MiB/s, done.\n",
      "Resolving deltas: 100% (17769/17769), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone herring (smdataparallel) forks repository for reference implementation of H\n",
    "!rm -rf DeepLearningExamples\n",
    "!git clone --recursive https://github.com/HerringForks/DeepLearningExamples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3.16xlarge\" # Other supported instance type: ml.p3.16xlarge\n",
    "instance_count = 2 # You can use 2, 4, 8 etc.\n",
    "docker_image = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image}:{tag}\" # YOUR_ECR_IMAGE_BUILT_WITH_ABOVE_DOCKER_FILE\n",
    "username = 'AWS'\n",
    "subnets = ['subnet-b3ec00f9'] # Should be same as Subnet used for FSx. Example: subnet-0f9XXXX\n",
    "security_group_ids = ['sg-fddcc4ae'] # Should be same as Security group used for FSx. sg-03ZZZZZZ\n",
    "job_name = 'tf2-smdataparallel-mrcnn-fsx-3' # This job name is used as prefix to the sagemaker training job. Makes it easy for your look for your training job in SageMaker Training job console.\n",
    "file_system_id='fs-02362f5e7db00888b' # FSx file system ID with your training dataset. Example: 'fs-0bYYYYYY'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "SM_DATA_ROOT = '/opt/ml/input/data/train'\n",
    "\n",
    "hyperparameters={\n",
    "    \"mode\": \"train\",\n",
    "    \"checkpoint\": '/'.join([SM_DATA_ROOT, 'model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603']), \n",
    "    \"eval_samples\": 5000,\n",
    "    \"init_learning_rate\": 0.04, \n",
    "    \"learning_rate_steps\": \"3750,5000\", \n",
    "    \"model_dir\": \"/opt/ml/model/\", \n",
    "    \"num_steps_per_eval\": 462,\n",
    "    \"total_steps\": 500,\n",
    "    \"train_batch_size\": 4,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"training_file_pattern\": '/'.join([SM_DATA_ROOT, 'train2017']), \n",
    "    \"validation_file_pattern\": '/'.join([SM_DATA_ROOT, 'val2017']), \n",
    "    \"val_json_file\": '/'.join([SM_DATA_ROOT, 'annotations/instances_val2017.json']),    \n",
    "    \"amp\": '',\n",
    "    \"use_batched_nms\": '',\n",
    "    \"xla\": '',\n",
    "    \"nouse_custom_box_proposals_op\": '',\n",
    "    \"seed\": 987\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm.py',\n",
    "                        role=role,\n",
    "                        image_uri=docker_image,\n",
    "                        source_dir='.',\n",
    "                        framework_version='2.3.1',\n",
    "                        py_version='py3',\n",
    "                        instance_count=instance_count,\n",
    "                        instance_type=instance_type,\n",
    "                        sagemaker_session=sagemaker_session,\n",
    "#                         subnets=subnets,\n",
    "                        hyperparameters=hyperparameters,\n",
    "#                         security_group_ids=security_group_ids,\n",
    "                        debugger_hook_config=False,\n",
    "                        # Training using SMDataParallel Distributed Training Framework\n",
    "                        distribution={'smdistributed':{\n",
    "                                            'dataparallel':{\n",
    "                                                    'enabled': True\n",
    "                                                 }\n",
    "                                          }\n",
    "                                      }\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure FSx Input for your SageMaker Training job\n",
    "# change to s3 \n",
    "# from sagemaker.inputs import FileSystemInput\n",
    "# file_system_directory_path='/fsx' # NOTE: '/fsx/' will be the root mount path. Example: '/fsx/mask_rcnn/PyTorch'\n",
    "# file_system_access_mode='rw'\n",
    "# file_system_type='FSxLustre'\n",
    "# train_fs = FileSystemInput(file_system_id=file_system_id,\n",
    "#                                     file_system_type=file_system_type,\n",
    "#                                     directory_path=file_system_directory_path,\n",
    "#                                     file_system_access_mode=file_system_access_mode)\n",
    "\n",
    "train_fs = sagemaker.inputs.TrainingInput('s3://coco2017-yianc-20210114/')\n",
    "data_channels = {'train': train_fs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-16 00:11:31 Starting - Starting the training job...\n",
      "2021-01-16 00:11:54 Starting - Launching requested ML instancesProfilerReport-1610755850: InProgress\n",
      "............\n",
      "2021-01-16 00:13:56 Starting - Preparing the instances for training......\n",
      "2021-01-16 00:14:56 Downloading - Downloading input data............................................................................................................................................................\n",
      "2021-01-16 00:40:56 Training - Downloading the training image...........................\u001b[34m2021-01-16 00:45:32.153363: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:32.160632: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:32.524279: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:32.645552: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:36,740 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:38.988606: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:39.001103: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:39.535332: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:39.705674: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:45,009 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:49,259 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:49,260 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:49,262 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:49,262 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.222.82\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:50,264 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:50,264 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.222.82\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:51,266 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:51,266 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.222.82\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:52,268 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:52,269 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.222.82\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:53,271 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:53,271 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.222.82\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:54,272 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:54,272 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.222.82\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:55,274 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:55,274 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.222.82\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:56,276 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:56,276 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.222.82\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:57,278 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:57,278 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.222.82\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:57,809 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:57,809 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:57,817 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:57,820 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:57,820 sagemaker-training-toolkit ERROR    Connection failed\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/sagemaker_training/smdataparallel.py\", line 303, in _can_connect\n",
      "    client.connect(host, port=port)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/paramiko/client.py\", line 368, in connect\n",
      "    raise NoValidConnectionsError(errors)\u001b[0m\n",
      "\u001b[34mparamiko.ssh_exception.NoValidConnectionsError: [Errno None] Unable to connect to port 22 on 10.0.203.160\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:57,822 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:58,288 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:58,373 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:58,373 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:58,373 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:58,373 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-01-16 00:45:58,384 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:58,831 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:58,908 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:58,908 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:58,908 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:58,909 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:58,909 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:58,909 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:58,909 sagemaker-training-toolkit INFO     instance type: ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34m2021-01-16 00:45:58,996 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"checkpoint\": \"/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603\",\n",
      "        \"nouse_custom_box_proposals_op\": \"\",\n",
      "        \"num_steps_per_eval\": 462,\n",
      "        \"seed\": 987,\n",
      "        \"amp\": \"\",\n",
      "        \"learning_rate_steps\": \"3750,5000\",\n",
      "        \"validation_file_pattern\": \"/opt/ml/input/data/train/val2017\",\n",
      "        \"eval_samples\": 5000,\n",
      "        \"init_learning_rate\": 0.04,\n",
      "        \"mode\": \"train\",\n",
      "        \"model_dir\": \"s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model\",\n",
      "        \"training_file_pattern\": \"/opt/ml/input/data/train/train2017\",\n",
      "        \"xla\": \"\",\n",
      "        \"eval_batch_size\": 8,\n",
      "        \"train_batch_size\": 4,\n",
      "        \"val_json_file\": \"/opt/ml/input/data/train/annotations/instances_val2017.json\",\n",
      "        \"total_steps\": 500,\n",
      "        \"use_batched_nms\": \"\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf2-smdataparallel-mrcnn-fsx-3\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"amp\":\"\",\"checkpoint\":\"/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603\",\"eval_batch_size\":8,\"eval_samples\":5000,\"init_learning_rate\":0.04,\"learning_rate_steps\":\"3750,5000\",\"mode\":\"train\",\"model_dir\":\"s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model\",\"nouse_custom_box_proposals_op\":\"\",\"num_steps_per_eval\":462,\"seed\":987,\"total_steps\":500,\"train_batch_size\":4,\"training_file_pattern\":\"/opt/ml/input/data/train/train2017\",\"use_batched_nms\":\"\",\"val_json_file\":\"/opt/ml/input/data/train/annotations/instances_val2017.json\",\"validation_file_pattern\":\"/opt/ml/input/data/train/val2017\",\"xla\":\"\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p3.16xlarge\"},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"amp\":\"\",\"checkpoint\":\"/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603\",\"eval_batch_size\":8,\"eval_samples\":5000,\"init_learning_rate\":0.04,\"learning_rate_steps\":\"3750,5000\",\"mode\":\"train\",\"model_dir\":\"s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model\",\"nouse_custom_box_proposals_op\":\"\",\"num_steps_per_eval\":462,\"seed\":987,\"total_steps\":500,\"train_batch_size\":4,\"training_file_pattern\":\"/opt/ml/input/data/train/train2017\",\"use_batched_nms\":\"\",\"val_json_file\":\"/opt/ml/input/data/train/annotations/instances_val2017.json\",\"validation_file_pattern\":\"/opt/ml/input/data/train/val2017\",\"xla\":\"\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf2-smdataparallel-mrcnn-fsx-3\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/source/sourcedir.tar.gz\",\"module_name\":\"DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--amp\",\"\",\"--checkpoint\",\"/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603\",\"--eval_batch_size\",\"8\",\"--eval_samples\",\"5000\",\"--init_learning_rate\",\"0.04\",\"--learning_rate_steps\",\"3750,5000\",\"--mode\",\"train\",\"--model_dir\",\"s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model\",\"--nouse_custom_box_proposals_op\",\"\",\"--num_steps_per_eval\",\"462\",\"--seed\",\"987\",\"--total_steps\",\"500\",\"--train_batch_size\",\"4\",\"--training_file_pattern\",\"/opt/ml/input/data/train/train2017\",\"--use_batched_nms\",\"\",\"--val_json_file\",\"/opt/ml/input/data/train/annotations/instances_val2017.json\",\"--validation_file_pattern\",\"/opt/ml/input/data/train/val2017\",\"--xla\",\"\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINT=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603\u001b[0m\n",
      "\u001b[34mSM_HP_NOUSE_CUSTOM_BOX_PROPOSALS_OP=\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_STEPS_PER_EVAL=462\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=987\u001b[0m\n",
      "\u001b[34mSM_HP_AMP=\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE_STEPS=3750,5000\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE_PATTERN=/opt/ml/input/data/train/val2017\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_SAMPLES=5000\u001b[0m\n",
      "\u001b[34mSM_HP_INIT_LEARNING_RATE=0.04\u001b[0m\n",
      "\u001b[34mSM_HP_MODE=train\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_FILE_PATTERN=/opt/ml/input/data/train/train2017\u001b[0m\n",
      "\u001b[34mSM_HP_XLA=\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_VAL_JSON_FILE=/opt/ml/input/data/train/annotations/instances_val2017.json\u001b[0m\n",
      "\u001b[34mSM_HP_TOTAL_STEPS=500\u001b[0m\n",
      "\u001b[34mSM_HP_USE_BATCHED_NMS=\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /usr/local/bin/python3.7 -m mpi4py DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm.py --amp  --checkpoint /opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_batch_size 8 --eval_samples 5000 --init_learning_rate 0.04 --learning_rate_steps 3750,5000 --mode train --model_dir s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --nouse_custom_box_proposals_op  --num_steps_per_eval 462 --seed 987 --total_steps 500 --train_batch_size 4 --training_file_pattern /opt/ml/input/data/train/train2017 --use_batched_nms  --val_json_file /opt/ml/input/data/train/annotations/instances_val2017.json --validation_file_pattern /opt/ml/input/data/train/val2017 --xla \n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:python /opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py --mode=train --checkpoint=/opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_samples=5000 --init_learning_rate=0.04 --learning_rate_steps=3750,5000 --model_dir=s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --num_steps_per_eval=462 --total_steps=500 --train_batch_size=4 --eval_batch_size=8 --training_file_pattern=/opt/ml/input/data/train/train2017/train*.tfrecord --validation_file_pattern=/opt/ml/input/data/train/val2017/val*.tfrecord --val_json_file=/opt/ml/input/data/train/annotations/instances_val2017.json --amp --use_batched_nms --xla  --nouse_custom_box_proposals_op --seed=987\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-01-16 00:46:00 Training - Training image download completed. Training in progress.\u001b[35m2021-01-16 00:46:00,392 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=180, name='orted', status='sleeping', started='00:45:59')]\u001b[0m\n",
      "\u001b[35m2021-01-16 00:46:00,392 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=180, name='orted', status='sleeping', started='00:45:59')]\u001b[0m\n",
      "\u001b[35m2021-01-16 00:46:00,393 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=180, name='orted', status='sleeping', started='00:45:59')]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Bootstrap : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Bootstrap : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Bootstrap : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Bootstrap : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Bootstrap : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Bootstrap : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Bootstrap : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Bootstrap : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO NET/Socket : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO NET/Socket : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO NET/Socket : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO NET/Socket : Using [0]eth0:10.0.203.160<0>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Bootstrap : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Bootstrap : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Bootstrap : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Bootstrap : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO NET/Socket : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO NET/Socket : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO NET/Socket : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO NET/Socket : Using [0]eth0:10.0.222.82<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Using network Socket\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Trees [0] 2/-1/-1->3->0|0->3->2/-1/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1 [2] -1/-1/-1->3->2|2->3->-1/-1/-1 [3] -1/-1/-1->3->2|2->3->-1/-1/-1 [4] 7/-1/-1->3->1|1->3->7/-1/-1 [5] 1/-1/-1->3->7|7->3->1/-1/-1 [6] 2/-1/-1->3->0|0->3->2/-1/-1 [7] 2/-1/-1->3->0|0->3->2/-1/-1 [8] -1/-1/-1->3->2|2->3->-1/-1/-1 [9] -1/-1/-1->3->2|2->3->-1/-1/-1 [10] 7/-1/-1->3->1|1->3->7/-1/-1 [11] 1/-1/-1->3->7|7->3->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1 [2] 3/-1/-1->2->1|1->2->3/-1/-1 [3] 3/-1/-1->2->1|1->2->3/-1/-1 [4] -1/-1/-1->2->6|6->2->-1/-1/-1 [5] 6/-1/-1->2->0|0->2->6/-1/-1 [6] 1/-1/-1->2->3|3->2->1/-1/-1 [7] 1/-1/-1->2->3|3->2->1/-1/-1 [8] 3/-1/-1->2->1|1->2->3/-1/-1 [9] 3/-1/-1->2->1|1->2->3/-1/-1 [10] -1/-1/-1->2->6|6->2->-1/-1/-1 [11] 6/-1/-1->2->0|0->2->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1 [2] 7/-1/-1->4->0|0->4->7/-1/-1 [3] 7/-1/-1->4->0|0->4->7/-1/-1 [4] 6/-1/-1->4->5|5->4->6/-1/-1 [5] 5/-1/-1->4->6|6->4->5/-1/-1 [6] -1/-1/-1->4->7|7->4->-1/-1/-1 [7] -1/-1/-1->4->7|7->4->-1/-1/-1 [8] 7/-1/-1->4->0|0->4->7/-1/-1 [9] 7/-1/-1->4->0|0->4->7/-1/-1 [10] 6/-1/-1->4->5|5->4->6/-1/-1 [11] 5/-1/-1->4->6|6->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1 [2] 1/-1/-1->5->6|6->5->1/-1/-1 [3] 1/-1/-1->5->6|6->5->1/-1/-1 [4] 4/-1/-1->5->7|7->5->4/-1/-1 [5] 7/-1/-1->5->4|4->5->7/-1/-1 [6] 6/-1/-1->5->1|1->5->6/-1/-1 [7] 6/-1/-1->5->1|1->5->6/-1/-1 [8] 1/-1/-1->5->6|6->5->1/-1/-1 [9] 1/-1/-1->5->6|6->5->1/-1/-1 [10] 4/-1/-1->5->7|7->5->4/-1/-1 [11] 7/-1/-1->5->4|4->5->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1 [2] 6/-1/-1->7->4|4->7->6/-1/-1 [3] 6/-1/-1->7->4|4->7->6/-1/-1 [4] 5/-1/-1->7->3|3->7->5/-1/-1 [5] 3/-1/-1->7->5|5->7->3/-1/-1 [6] 4/-1/-1->7->6|6->7->4/-1/-1 [7] 4/-1/-1->7->6|6->7->4/-1/-1 [8] 6/-1/-1->7->4|4->7->6/-1/-1 [9] 6/-1/-1->7->4|4->7->6/-1/-1 [10] 5/-1/-1->7->3|3->7->5/-1/-1 [11] 3/-1/-1->7->5|5->7->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1 [2] 2/-1/-1->1->5|5->1->2/-1/-1 [3] 2/-1/-1->1->5|5->1->2/-1/-1 [4] 3/-1/-1->1->0|0->1->3/-1/-1 [5] -1/-1/-1->1->3|3->1->-1/-1/-1 [6] 5/-1/-1->1->2|2->1->5/-1/-1 [7] 5/-1/-1->1->2|2->1->5/-1/-1 [8] 2/-1/-1->1->5|5->1->2/-1/-1 [9] 2/-1/-1->1->5|5->1->2/-1/-1 [10] 3/-1/-1->1->0|0->1->3/-1/-1 [11] -1/-1/-1->1->3|3->1->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1 [2] 5/-1/-1->6->7|7->6->5/-1/-1 [3] 5/-1/-1->6->7|7->6->5/-1/-1 [4] 2/-1/-1->6->4|4->6->2/-1/-1 [5] 4/-1/-1->6->2|2->6->4/-1/-1 [6] 7/-1/-1->6->5|5->6->7/-1/-1 [7] 7/-1/-1->6->5|5->6->7/-1/-1 [8] 5/-1/-1->6->7|7->6->5/-1/-1 [9] 5/-1/-1->6->7|7->6->5/-1/-1 [10] 2/-1/-1->6->4|4->6->2/-1/-1 [11] 4/-1/-1->6->2|2->6->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 00/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 01/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 02/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 03/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 04/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 05/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 06/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 07/12 :    0   3   2   1   5   6   7   4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 08/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 09/12 :    0   4   7   6   5   1   2   3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 10/12 :    0   1   3   7   5   4   6   2\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 11/12 :    0   2   6   4   5   7   3   1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->-1|-1->0->3/-1/-1 [2] 4/-1/-1->0->-1|-1->0->4/-1/-1 [3] 4/-1/-1->0->-1|-1->0->4/-1/-1 [4] 1/-1/-1->0->-1|-1->0->1/-1/-1 [5] 2/-1/-1->0->-1|-1->0->2/-1/-1 [6] 3/-1/-1->0->-1|-1->0->3/-1/-1 [7] 3/-1/-1->0->-1|-1->0->3/-1/-1 [8] 4/-1/-1->0->-1|-1->0->4/-1/-1 [9] 4/-1/-1->0->-1|-1->0->4/-1/-1 [10] 1/-1/-1->0->-1|-1->0->1/-1/-1 [11] 2/-1/-1->0->-1|-1->0->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 00 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 01 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 02 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 02 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 02 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 02 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 02 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 02 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 02 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 02 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 02 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 02 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 02 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 02 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 02 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 02 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 02 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 03 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 03 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 03 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 03 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 03 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 03 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 03 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 03 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 03 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 03 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 03 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 03 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 03 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 03 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 03 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 04 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 04 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 04 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 04 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 04 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 04 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 04 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 04 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 04 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 04 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 04 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 04 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 04 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 04 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 04 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 05 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 05 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 05 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 05 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 05 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 05 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 05 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 05 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 05 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 05 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 05 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 05 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 05 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 05 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 05 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 06 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 06 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 06 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 06 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 06 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 06 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 06 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 06 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 06 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 06 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 06 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 06 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 06 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 06 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 06 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 07 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 07 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 07 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 07 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 07 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 07 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 07 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 07 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 07 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 07 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 07 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 07 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 07 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 07 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 07 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 08 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 08 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 08 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 08 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 08 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 08 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 08 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 08 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 08 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 08 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 08 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 08 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 08 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 08 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 08 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 09 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 09 : 0[170] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 09 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 09 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 09 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 09 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 09 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 09 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 09 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 09 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 09 : 4[1b0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 09 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 09 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 09 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 09 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 10 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 10 : 0[170] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 10 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 10 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 10 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 10 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 10 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 10 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 10 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 10 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 10 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 10 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 10 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 10 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 10 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO comm 0x564dae1e8530 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO comm 0x562c887f5b80 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO comm 0x55c46c873130 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 11 : 2[190] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO comm 0x5562784b16d0 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO comm 0x555eb3b5bb90 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 11 : 0[170] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO comm 0x55c2f1a85330 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO comm 0x5594151aa8e0 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO comm 0x55ede05097f0 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 11 : 1[180] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 11 : 4[1b0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 11 : 3[1a0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 11 : 5[1c0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 11 : 6[1d0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 11 : 7[1e0] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 11 : 1[180] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 11 : 2[190] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 11 : 4[1b0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 11 : 3[1a0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 11 : 6[1d0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 11 : 5[1c0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 11 : 7[1e0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO comm 0x55b1dfbb5730 rank 1 nranks 8 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO comm 0x556e1e6d8100 rank 0 nranks 8 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO comm 0x55da884ac0b0 rank 2 nranks 8 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO comm 0x560554a0c570 rank 4 nranks 8 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO comm 0x55ab22271e90 rank 3 nranks 8 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO comm 0x564b44e10c70 rank 5 nranks 8 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO comm 0x55e68b36c930 rank 6 nranks 8 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO 12 coll channels, 16 p2p channels, 2 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO comm 0x55c94b407ff0 rank 7 nranks 8 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 00/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 01/02 :    0   3   2   1   5   6   7   4   8  11  10   9  13  14  15  12\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Trees [0] 1/-1/-1->2->3|3->2->1/-1/-1 [1] 1/-1/-1->2->3|3->2->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Trees [0] 5/-1/-1->1->2|2->1->5/-1/-1 [1] 5/-1/-1->1->2|2->1->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Trees [0] 2/8/-1->3->0|0->3->2/8/-1 [1] 2/-1/-1->3->0|0->3->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Trees [0] -1/-1/-1->4->7|7->4->-1/-1/-1 [1] -1/-1/-1->4->7|7->4->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Trees [0] 3/-1/-1->0->-1|-1->0->3/-1/-1 [1] 3/-1/-1->0->11|11->0->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Trees [0] 6/-1/-1->5->1|1->5->6/-1/-1 [1] 6/-1/-1->5->1|1->5->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Trees [0] 12/-1/-1->15->14|14->15->12/-1/-1 [1] 12/-1/-1->15->14|14->15->12/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Trees [0] 4/-1/-1->7->6|6->7->4/-1/-1 [1] 4/-1/-1->7->6|6->7->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Trees [0] 15/-1/-1->14->13|13->14->15/-1/-1 [1] 15/-1/-1->14->13|13->14->15/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Trees [0] -1/-1/-1->12->15|15->12->-1/-1/-1 [1] -1/-1/-1->12->15|15->12->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Trees [0] 14/-1/-1->13->9|9->13->14/-1/-1 [1] 14/-1/-1->13->9|9->13->14/-1/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Trees [0] 9/-1/-1->10->11|11->10->9/-1/-1 [1] 9/-1/-1->10->11|11->10->9/-1/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Trees [0] 13/-1/-1->9->10|10->9->13/-1/-1 [1] 13/-1/-1->9->10|10->9->13/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Trees [0] 11/-1/-1->8->3|3->8->11/-1/-1 [1] 11/-1/-1->8->-1|-1->8->11/-1/-1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Trees [0] 10/-1/-1->11->8|8->11->10/-1/-1 [1] 10/0/-1->11->8|8->11->10/0/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 00 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 00 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 00 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 00 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 00 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 00 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 00 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 00 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 00 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 00 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 00 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 00 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 00 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 00 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 00 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 00 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 00 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 00 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 00 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 00 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 00 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 00 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 00 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 00 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 00 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 00 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 00 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 00 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 00 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 01 : 14[1d0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 01 : 13[1c0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 00 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 01 : 1[180] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 01 : 2[190] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 01 : 9[180] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 00 : 8[170] -> 3[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 01 : 6[1d0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 01 : 5[1c0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 01 : 15[1e0] -> 12[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 01 : 10[190] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 01 : 7[1e0] -> 4[1b0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 01 : 11[1a0] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO Channel 01 : 14[1d0] -> 13[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 01 : 12[1b0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO Channel 01 : 13[1c0] -> 9[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO Channel 01 : 1[180] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO Channel 01 : 9[180] -> 10[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO Channel 01 : 15[1e0] -> 14[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 01 : 0[170] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO Channel 01 : 5[1c0] -> 1[180] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO Channel 01 : 6[1d0] -> 5[1c0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO Channel 01 : 7[1e0] -> 6[1d0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:210:210 [5] NCCL INFO comm 0x55ede31da8d0 rank 13 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO Channel 01 : 10[190] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:214:214 [6] NCCL INFO comm 0x559417e7b9c0 rank 14 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:206:206 [1] NCCL INFO comm 0x564db0eb9610 rank 9 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:201:201 [5] NCCL INFO comm 0x564b47ae1c50 rank 5 nranks 16 cudaDev 5 busId 1c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:204:204 [6] NCCL INFO comm 0x55e68e03da10 rank 6 nranks 16 cudaDev 6 busId 1d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO Channel 01 : 12[1b0] -> 15[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:207:207 [4] NCCL INFO comm 0x555eb682cc70 rank 12 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:213:213 [7] NCCL INFO comm 0x55c2f4756410 rank 15 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 00 : 3[1a0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 01 : 3[1a0] -> 2[190] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO Channel 01 : 2[190] -> 3[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO Channel 01 : 3[1a0] -> 0[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:197:197 [1] NCCL INFO comm 0x55b1e2886810 rank 1 nranks 16 cudaDev 1 busId 180 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:198:198 [2] NCCL INFO comm 0x55da8b17d190 rank 2 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 01 : 4[1b0] -> 8[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO Channel 01 : 8[170] -> 11[1a0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:205:205 [2] NCCL INFO comm 0x55627b1827b0 rank 10 nranks 16 cudaDev 2 busId 190 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 01 : 11[1a0] -> 8[170] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:195:195 [3] NCCL INFO comm 0x55ab24f42f70 rank 3 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO Channel 01 : 4[1b0] -> 7[1e0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:205:205 [7] NCCL INFO comm 0x55c94e0d8fd0 rank 7 nranks 16 cudaDev 7 busId 1e0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:200:200 [4] NCCL INFO comm 0x5605576dd650 rank 4 nranks 16 cudaDev 4 busId 1b0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:209:209 [0] NCCL INFO comm 0x562c8b4c6c60 rank 8 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 01 : 0[170] -> 11[1a0] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO Channel 01 : 11[1a0] -> 0[170] [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:196:196 [0] NCCL INFO comm 0x556e213a91e0 rank 0 nranks 16 cudaDev 0 busId 170 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:208:208 [3] NCCL INFO comm 0x55c46f544210 rank 11 nranks 16 cudaDev 3 busId 1a0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running smdistributed.dataparallel v1.0.0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : SageMaker Distributed Data Parallel successfully initialized ...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : [train] AMP is activated - Experiment Feature\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : [train] XLA is activated - Experiment Feature\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : Using Dataset Sharding with SageMaker Distributed Data Parallel\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : [ROI OPs] Using Batched NMS... Scope: multilevel_propose_rois/level_2/\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : [ROI OPs] Using Batched NMS... Scope: multilevel_propose_rois/level_3/\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : [ROI OPs] Using Batched NMS... Scope: multilevel_propose_rois/level_4/\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : [ROI OPs] Using Batched NMS... Scope: multilevel_propose_rois/level_5/\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : [ROI OPs] Using Batched NMS... Scope: multilevel_propose_rois/level_6/\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : [Training Compute Statistics] 570.6 GFLOPS/image\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] DEBUG   : New Metric Registered: `L2 loss`, Aggregator: StandardMeter, Scope: LoggingScope.ITER, Distributed Strategy: DistributedStrategy.NONE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] DEBUG   : New Metric Registered: `Mask loss`, Aggregator: StandardMeter, Scope: LoggingScope.ITER, Distributed Strategy: DistributedStrategy.NONE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] DEBUG   : New Metric Registered: `Total loss`, Aggregator: StandardMeter, Scope: LoggingScope.ITER, Distributed Strategy: DistributedStrategy.NONE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] DEBUG   : New Metric Registered: `RPN box loss`, Aggregator: StandardMeter, Scope: LoggingScope.ITER, Distributed Strategy: DistributedStrategy.NONE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] DEBUG   : New Metric Registered: `RPN score loss`, Aggregator: StandardMeter, Scope: LoggingScope.ITER, Distributed Strategy: DistributedStrategy.NONE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] DEBUG   : New Metric Registered: `RPN total loss`, Aggregator: StandardMeter, Scope: LoggingScope.ITER, Distributed Strategy: DistributedStrategy.NONE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] DEBUG   : New Metric Registered: `FastRCNN class loss`, Aggregator: StandardMeter, Scope: LoggingScope.ITER, Distributed Strategy: DistributedStrategy.NONE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] DEBUG   : New Metric Registered: `FastRCNN box loss`, Aggregator: StandardMeter, Scope: LoggingScope.ITER, Distributed Strategy: DistributedStrategy.NONE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] DEBUG   : New Metric Registered: `FastRCNN total loss`, Aggregator: StandardMeter, Scope: LoggingScope.ITER, Distributed Strategy: DistributedStrategy.NONE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] DEBUG   : New Metric Registered: `Learning rate`, Aggregator: StandardMeter, Scope: LoggingScope.ITER, Distributed Strategy: DistributedStrategy.NONE\u001b[0m\n",
      "\n",
      "2021-01-16 00:46:42 Uploading - Uploading generated training model\u001b[34m[1,10]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    return fn(*args)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _run_fn\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    target_list, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:#011 [[{{node list_files/assert_not_empty/Assert}}]]\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1212, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1511, in _train_with_estimator_spec\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    log_step_count_steps=log_step_count_steps) as mon_sess:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 749, in __init__\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    self._sess = _RecoverableSession(self._coordinated_creator)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1231, in __init__\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    _WrappedSession.__init__(self, self._create_session())\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1236, in _create_session\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    return self._sess_creator.create_session()\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 909, in create_session\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    hook.after_create_session(self.tf_sess, self.coord)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/util.py\", line 86, in after_create_session\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    session.run(self._initializer)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    run_metadata_ptr)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    feed_dict_tensor, options, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    raise type(e)(node_def, op, message)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:#011 [[node list_files/assert_not_empty/Assert (defined at opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py:97) ]]\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:Original stack trace for 'list_files/assert_not_empty/Assert':\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1205, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1041, in _get_features_and_labels_from_input_fn\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    self._call_input_fn(input_fn, mode))\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr[1,10]<stdout>:/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1134, in _call_input_fn\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    return input_fn(**kwargs)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py\", line 97, in __call__\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    shuffle=False\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1115, in list_files\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    condition, [message], summarize=1, name=\"assert_not_empty\")\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    return target(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 247, in wrapped\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    return _add_should_use_warning(fn(*args, **kwargs),\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 167, in Assert\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 63, in _assert\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    name=name)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    attrs=attr_protos, op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3485, in _create_op_internal\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:    self._traceback = tf_stack.extract_stack()\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : Job finished with status: `SUCCESS`\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    return fn(*args)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _run_fn\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    target_list, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:#011 [[{{node list_files/assert_not_empty/Assert}}]]\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1212, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1511, in _train_with_estimator_spec\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    log_step_count_steps=log_step_count_steps) as mon_sess:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 749, in __init__\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    self._sess = _RecoverableSession(self._coordinated_creator)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1231, in __init__\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    _WrappedSession.__init__(self, self._create_session())\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1236, in _create_session\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    return self._sess_creator.create_session()\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 909, in create_session\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    hook.after_create_session(self.tf_sess, self.coord)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/util.py\", line 86, in after_create_session\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    session.run(self._initializer)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    run_metadata_ptr)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    feed_dict_tensor, options, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    raise type(e)(node_def, op, message)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:#011 [[node list_files/assert_not_empty/Assert (defined at opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py:97) ]]\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:Original stack trace for 'list_files/assert_not_empty/Assert':\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1205, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1041, in _get_features_and_labels_from_input_fn\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    self._call_input_fn(input_fn, mode))\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1134, in _call_input_fn\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    return input_fn(**kwargs)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py\", line 97, in __call__\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    shuffle=False\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1115, in list_files\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    condition, [message], summarize=1, name=\"assert_not_empty\")\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    return target(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 247, in wrapped\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    return _add_should_use_warning(fn(*args, **kwargs),\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 167, in Assert\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 63, in _assert\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    name=name)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    attrs=attr_protos, op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3485, in _create_op_internal\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:    self._trac[1,9]<stdout>:eback = tf_stack.extract_stack()\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    return fn(*args)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _run_fn\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    target_list, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:#011 [[{{node list_files/assert_not_empty/Assert}}]]\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1212, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1511, in _train_with_estimator_spec\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    log_step_count_steps=log_step_count_steps) as mon_sess:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 749, in __init__\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    self._sess = _RecoverableSession(self._coordinated_creator)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1231, in __init__\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    _WrappedSession.__init__(self, self._create_session())\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1236, in _create_session\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    return self._sess_creator.create_session()\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 909, in create_session\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    hook.after_create_session(self.tf_sess, self.coord)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/util.py\", line 86, in after_create_session\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    session.run(self._initializer)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    run_metadata_ptr)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    feed_dict_tensor, options, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    raise type(e)(node_def, op, message)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:#011 [[node list_files/assert_not_empty/Assert (defined at opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py:97) ]]\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:Original stack trace for 'list_files/assert_not_empty/Assert':\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1205, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1041, in _get_features_and_labels_from_input_fn\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    self._call_input_fn(input_fn, mode))\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr[1,14]<stdout>:/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1134, in _call_input_fn\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    return input_fn(**kwargs)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py\", line 97, in __call__\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    shuffle=False\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1115, in list_files\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    condition, [message], summarize=1, name=\"assert_not_empty\")\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    return target(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 247, in wrapped\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    return _add_should_use_warning(fn(*args, **kwargs),\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 167, in Assert\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 63, in _assert\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    name=name)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    attrs=attr_protos, op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3485, in _create_op_internal\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:    self._traceback = tf_stack.extract_stack()\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    return fn(*args)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _run_fn\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    target_list, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:#011 [[{{node list_files/assert_not_empty/Assert}}]]\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1212, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1511, in _train_with_estimator_spec\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    log_step_count_steps=log_step_count_steps) as mon_sess:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 749, in __init__\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    self._sess = _RecoverableSession(self._coordinated_creator)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1231, in __init__\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    _WrappedSession.__init__(self, self._create_session())\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1236, in _create_session\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    return self._sess_creator.create_session()\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 909, in create_session\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    hook.after_create_session(self.tf_sess, self.coord)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/util.py\", line 86, in after_create_session\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    session.run(self._initializer)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    run_metadata_ptr)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    feed_dict_tensor, options, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    raise type(e)(node_def, op, message)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:#011 [[node list_files/assert_not_empty/Assert (defined at opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py:97) ]]\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:Original stack trace for 'list_files/assert_not_empty/Assert':\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1205, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1041, in _get_features_and_labels_from_input_fn\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    self._call_input_fn(input_fn, mode))\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr[1,12]<stdout>:/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1134, in _call_input_fn\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    return input_fn(**kwargs)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py\", line 97, in __call__\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    shuffle=False\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1115, in list_files\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    condition, [message], summarize=1, name=\"assert_not_empty\")\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    return target(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 247, in wrapped\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    return _add_should_use_warning(fn(*args, **kwargs),\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 167, in Assert\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 63, in _assert\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    name=name)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    attrs=attr_protos, op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3485, in _create_op_internal\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:    self._traceback = tf_stack.extract_stack()\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    return fn(*args)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _run_fn\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    target_list, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:#011 [[{{node list_files/assert_not_empty/Assert}}]]\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1212, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1511, in _train_with_estimator_spec\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    log_step_count_steps=log_step_count_steps) as mon_sess:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 749, in __init__\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    self._sess = _RecoverableSession(self._coordinated_creator)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1231, in __init__\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    _WrappedSession.__init__(self, self._create_session())\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1236, in _create_session\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    return self._sess_creator.create_session()\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 909, in create_session\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    hook.after_create_session(self.tf_sess, self.coord)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/util.py\", line 86, in after_create_session\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    session.run(self._initializer)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    run_metadata_ptr)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    feed_dict_tensor, options, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    raise type(e)(node_def, op, message)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:#011 [[node list_files/assert_not_empty/Assert (defined at opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py:97) ]]\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:Original stack trace for 'list_files/assert_not_empty/Assert':\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1205, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1041, in _get_features_and_labels_from_input_fn\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    self._call_input_fn(input_fn, mode))\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1134, in _call_input_fn\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    return input_fn(**kwargs)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py\", line 97, in __call__\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    shuffle=False\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1115, in list_files\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    condition, [message], summarize=1, name=\"assert_not_empty\")\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    return target(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 247, in wrapped\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    return _add_should_use_warning(fn(*args, **kwargs),\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 167, in Assert\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 63, in _assert\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    name=name)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    attrs=attr_protos, op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3485, in _create_op_internal\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:    self._traceback = tf_stack.extract_stack()\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    return fn(*args)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _run_fn\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    target_list, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:#011 [[{{node list_files/assert_not_empty/Assert}}]]\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1212, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1511, in _train_with_estimator_spec\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    log_step_count_steps=log_step_count_steps) as mon_sess:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 749, in __init__\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    self._sess = _RecoverableSession(self._coordinated_creator)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1231, in __init__\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    _WrappedSession.__init__(self, self._create_session())\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1236, in _create_session\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    return self._sess_creator.create_session()\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 909, in create_session\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    hook.after_create_session(self.tf_sess, self.coord)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/util.py\", line 86, in after_create_session\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[1,11]<stdout>:    session.run(self._initializer)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    run_metadata_ptr)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    feed_dict_tensor, options, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    raise type(e)(node_def, op, message)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:#011 [[node list_files/assert_not_empty/Assert (defined at opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py:97) ]]\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:Original stack trace for 'list_files/assert_not_empty/Assert':\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1205, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1041, in _get_features_and_labels_from_input_fn\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    self._call_input_fn(input_fn, mode))\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr[1,11]<stdout>:/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1134, in _call_input_fn\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    return input_fn(**kwargs)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py\", line 97, in __call__\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    shuffle=False\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1115, in list_files\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    condition, [message], summarize=1, name=\"assert_not_empty\")\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    return target(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 247, in wrapped\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    return _add_should_use_warning(fn(*args, **kwargs),\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 167, in Assert\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 63, in _assert\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    name=name)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    attrs=attr_protos, op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3485, in _create_op_internal\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:    self._traceback = tf_stack.extract_stack()\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    return fn(*args)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _run_fn\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    target_list, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:#011 [[{{node list_files/assert_not_empty/Assert}}]]\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1212, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1511, in _train_with_estimator_spec\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    log_step_count_steps=log_step_count_steps) as mon_sess:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 749, in __init__\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    self._sess = _RecoverableSession(self._coordinated_creator)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1231, in __init__\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    _WrappedSession.__init__(self, self._create_session())\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1236, in _create_session\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    return self._sess_creator.create_session()\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 909, in create_session\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    hook.after_create_session(self.tf_sess, self.coord)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/util.py\", line 86, in after_create_session\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    session.run(self._initializer)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    run_metadata_ptr)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    feed_dict_tensor, options, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    raise type(e)(node_def, op, message)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:#011 [[node list_files/assert_not_empty/Assert (defined at opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py:97) ]]\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:Original stack trace for 'list_files/assert_not_empty/Assert':\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1205, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1041, in _get_features_and_labels_from_input_fn\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    self._call_input_fn(input_fn, mode))\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1134, in _call_input_fn\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    return input_fn(**kwargs)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py\", line 97, in __call__\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    shuffle=False\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1115, in list_files\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    condition, [message], summarize=1, name=\"assert_not_empty\")\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    return target(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 247, in wrapped\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    return _add_should_use_warning(fn(*args, **kwargs),\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 167, in Assert\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 63, in _assert\u001b[0m\n",
      "\u001b[34m2021-01-16 00:46:39,267 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /usr/local/bin/python3.7 -m mpi4py DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm.py --amp  --checkpoint /opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_batch_size 8 --eval_samples 5000 --init_learning_rate 0.04 --learning_rate_steps 3750,5000 --mode train --model_dir s3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model --nouse_custom_box_proposals_op  --num_steps_per_eval 462 --seed 987 --total_steps 500 --train_batch_size 4 --training_file_pattern /opt/ml/input/data/train/train2017 --use_batched_nms  --val_json_file /opt/ml/input/data/train/annotations/instances_val2017.json --validation_file_pattern /opt/ml/input/data/train/val2017 --xla \"\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.203.160' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    name=name)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    attrs=attr_protos, op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3485, in _create_op_internal\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:    self._traceback = tf_stack.extract_stack()\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    return fn(*args)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _run_fn\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    target_list, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:#011 [[{{node list_files/assert_not_empty/Assert}}]]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-01-16 00:46:00.742761: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-01-16 00:46:00.742761: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-01-16 00:46:00.742775: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-01-16 00:46:00.742952: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-01-16 00:46:00.742952: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-01-16 00:46:00.742952: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-01-16 00:46:00.744184: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-01-16 00:46:00.744321: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-01-16 00:46:00.744773: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-01-16 00:46:00.744920: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-01-16 00:46:00.756001: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-01-16 00:46:00.756232: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-01-16 00:46:00.766265: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-01-16 00:46:00.766450: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-01-16 00:46:00.771587: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-01-16 00:46:00.771729: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-01-16 00:46:00.784672: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-01-16 00:46:00.784908: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-01-16 00:46:00.785345: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-01-16 00:46:00.785421: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-01-16 00:46:00.785555: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-01-16 00:46:00.785862: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-01-16 00:46:00.786612: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-01-16 00:46:00.787743: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-01-16 00:46:00.787776: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-01-16 00:46:00.787776: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-01-16 00:46:00.787884: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-01-16 00:46:00.787949: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-01-16 00:46:00.787949: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-01-16 00:46:00.792891: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-01-16 00:46:00.793048: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1212, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1511, in _train_with_estimator_spec\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    log_step_count_steps=log_step_count_steps) as mon_sess:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 604, in MonitoredTrainingSession\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1038, in __init__\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    stop_grace_period_secs=stop_grace_period_secs)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 749, in __init__\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    self._sess = _RecoverableSession(self._coordinated_creator)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1231, in __init__\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    _WrappedSession.__init__(self, self._create_session())\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1236, in _create_session\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    return self._sess_creator.create_session()\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py\", line 909, in create_session\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    hook.after_create_session(self.tf_sess, self.coord)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/util.py\", line 86, in after_create_session\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    session.run(self._initializer)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 958, in run\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    run_metadata_ptr)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1181, in _run\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    feed_dict_tensor, options, run_metadata)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1359, in _do_run\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    run_metadata)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    raise type(e)(node_def, op, message)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [No files matched pattern: /opt/ml/input/data/train/train2017/train*.tfrecord]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:#011 [[node list_files/assert_not_empty/Assert (defined at opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py:97) ]]\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:Original stack trace for 'list_files/assert_not_empty/Assert':\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 169, in <module>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    app.run(main)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    _run_main(main, args)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    sys.exit(main(argv))\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 159, in main\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    run_executer(RUN_CONFIG, train_input_fn, eval_input_fn)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_main.py\", line 74, in run_executer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    eval_input_fn=eval_input_fn\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/distributed_executer.py\", line 302, in train\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    skip_checkpoint_variables=self._runtime_config.skip_checkpoint_variables\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 350, in train\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    loss = self._train_model(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1179, in _train_model\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    return self._train_model_default(input_fn, hooks, saving_listeners)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1205, in _train_model_default\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1041, in _get_features_and_labels_from_input_fn\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    self._call_input_fn(input_fn, mode))\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1134, in _call_input_fn\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    return input_fn(**kwargs)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"opt/ml/code/DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn/dataloader.py\", line 97, in __call__\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    shuffle=False\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1115, in list_files\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    condition, [message], summarize=1, name=\"assert_not_empty\")\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    return target(*args, **kwargs)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 247, in wrapped\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    return _add_should_use_warning(fn(*args, **kwargs),\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 167, in Assert\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 63, in _assert\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    name=name)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    attrs=attr_protos, op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3485, in _create_op_internal\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    op_def=op_def)\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:  File \"usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:    self._traceback = tf_stack.extract_stack()\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.203.160' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-01-16 00:46:00.742761: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-01-16 00:46:00.742761: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-01-16 00:46:00.742775: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-01-16 00:46:00.742952: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-01-16 00:46:00.742952: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-01-16 00:46:00.742952: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-01-16 00:46:00.744184: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-01-16 00:46:00.744321: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-01-16 00:46:00.744773: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-01-16 00:46:00.744920: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-01-16 00:46:00.756001: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-01-16 00:46:00.756232: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-01-16 00:46:00.766265: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-01-16 00:46:00.766450: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-01-16 00:46:00.771587: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-01-16 00:46:00.771729: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-01-16 00:46:00.784672: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-01-16 00:46:00.784908: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-01-16 00:46:00.785345: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-01-16 00:46:00.785421: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-01-16 00:46:00.785555: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-01-16 00:46:00.785862: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-01-16 00:46:00.786612: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-01-16 00:46:00.787743: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-01-16 00:46:00.787776: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-01-16 00:46:00.787776: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-01-16 00:46:00.787884: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-01-16 00:46:00.787949: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-01-16 00:46:00.787949: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-01-16 00:46:00.792891: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-01-16 00:46:00.793048: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-01-16 00:46:00.798744: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-01-16 00:46:00.799755: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-01-16 00:46:00.799907: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-01-16 00:46:00.808859: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-01-16 00:46:00.813768: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-01-16 00:46:00.828826: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-01-16 00:46:00.828974: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-01-16 00:46:00.829039: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-01-16 00:46:00.829337: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-01-16 00:46:00.829554: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-01-16 00:46:00.829716: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-01-16 00:46:00.836077: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-01-16 00:46:00.840305: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-01-16 00:46:00.840542: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-01-16 00:46:00.841211: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-01-16 00:46:00.871742: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-01-16 00:46:00.882578: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 997, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  visible_device_list: \"2\"\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-01-16 00:46:00.798744: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-01-16 00:46:00.799755: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-01-16 00:46:00.799907: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-01-16 00:46:00.808859: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-01-16 00:46:00.813768: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-01-16 00:46:00.828826: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-01-16 00:46:00.828974: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-01-16 00:46:00.829039: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-01-16 00:46:00.829337: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-01-16 00:46:00.829554: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-01-16 00:46:00.829716: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-01-16 00:46:00.836077: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-01-16 00:46:00.840305: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-01-16 00:46:00.840542: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-01-16 00:46:00.841211: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-01-16 00:46:00.871742: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-01-16 00:46:00.882578: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 997, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  visible_device_list: \"2\"\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 1000, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  visible_device_list: \"5\"\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 1001, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  visible_device_list: \"6\"\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 1002, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  visible_device_list: \"7\"\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 998, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  visible_device_list: \"3\"\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 989, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  visible_device_list: \"2\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 991, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  visible_device_list: \"4\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 1000, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  visible_device_list: \"5\"\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 992, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  visible_device_list: \"5\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 996, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 1001, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  visible_device_list: \"6\"\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 1002, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  visible_device_list: \"1\"\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 993, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  visible_device_list: \"6\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 988, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  visible_device_list: \"1\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  visible_device_list: \"7\"\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 994, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  visible_device_list: \"7\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 995, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  visible_device_list: \"0\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 999, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  visible_device_list: \"4\"\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 998, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  visible_device_list: \"3\"\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 990, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 989, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  visible_device_list: \"3\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  visible_device_list: \"2\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 991, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  visible_device_list: \"4\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 987, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  visible_device_list: \"0\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 992, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  visible_device_list: \"5\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 996, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  visible_device_list: \"1\"\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-01-16 00:46:34.899309: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-01-16 00:46:34.910036: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-01-16 00:46:34.920785: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-01-16 00:46:34.927155: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-01-16 00:46:34.939535: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-01-16 00:46:35.003961: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-01-16 00:46:35.167511: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-01-16 00:46:35.205206: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-01-16 00:46:36.097604: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-01-16 00:46:36.140081: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-01-16 00:46:36.153095: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-01-16 00:46:36.181658: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-01-16 00:46:36.191785: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-01-16 00:46:36.198269: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-01-16 00:46:36.216236: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 993, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  visible_device_list: \"6\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 988, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  visible_device_list: \"1\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mMPI_ABORT was invoked on rank 10 in communicator MPI COMMUNICATOR 5 DUP FROM 0\u001b[0m\n",
      "\u001b[34mwith errorcode 1.\n",
      "\u001b[0m\n",
      "\u001b[34mNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\u001b[0m\n",
      "\u001b[34mYou may or may not see output from other processes, depending on\u001b[0m\n",
      "\u001b[34mexactly when Open MPI kills them.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:signal 15 received by process with PID 196\u001b[0m\n",
      "\u001b[34m[ip-10-0-222-82.us-west-2.compute.internal:00169] 7 more processes have sent help message help-mpi-api.txt / mpi-abort\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 994, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  visible_device_list: \"7\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[ip-10-0-222-82.us-west-2.compute.internal:00169] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n",
      "\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 995, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  visible_device_list: \"0\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 999, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  visible_device_list: \"4\"\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 990, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  visible_device_list: \"3\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-us-west-2-230755935769/tf2-smdataparallel-mrcnn-fsx-3/model', '_tf_random_seed': 987, '_save_summary_steps': None, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': intra_op_parallelism_threads: 1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:inter_op_parallelism_threads: 8\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:gpu_options {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  visible_device_list: \"0\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  force_gpu_compatible: true\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:allow_soft_placement: true\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:graph_options {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  optimizer_options {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    global_jit_level: ON_1\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  rewrite_options {\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    meta_optimizer_iterations: TWO\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    auto_mixed_precision: ON\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  }\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': None, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Done calling model_fn.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-01-16 00:46:34.899309: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-01-16 00:46:34.910036: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-01-16 00:46:34.920785: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-01-16 00:46:34.927155: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-01-16 00:46:34.939535: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-01-16 00:46:35.003961: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:tensorflow:Graph was finalized.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-01-16 00:46:35.167511: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-01-16 00:46:35.205206: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-01-16 00:46:36.097604: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-01-16 00:46:36.140081: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-01-16 00:46:36.153095: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-01-16 00:46:36.181658: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-01-16 00:46:36.191785: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-01-16 00:46:36.198269: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-01-16 00:46:36.216236: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1641] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:INFO:tensorflow:Done running local_init_op.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mMPI_ABORT was invoked on rank 10 in communicator MPI COMMUNICATOR 5 DUP FROM 0\u001b[0m\n",
      "\u001b[34mwith errorcode 1.\n",
      "\u001b[0m\n",
      "\u001b[34mNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\u001b[0m\n",
      "\u001b[34mYou may or may not see output from other processes, depending on\u001b[0m\n",
      "\u001b[34mexactly when Open MPI kills them.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:signal 15 received by process with PID 196\u001b[0m\n",
      "\u001b[34m[ip-10-0-222-82.us-west-2.compute.internal:00169] 7 more processes have sent help message help-mpi-api.txt / mpi-abort\u001b[0m\n",
      "\u001b[34m[ip-10-0-222-82.us-west-2.compute.internal:00169] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2021-01-16 00:46:39,274 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\n",
      "2021-01-16 00:47:13 Failed - Training job failed\n",
      "ProfilerReport-1610755850: Stopping\n",
      "\u001b[35m2021-01-16 00:47:09,299 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2021-01-16 00:47:09,299 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[35mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[35mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[35m2021-01-16 00:47:09,299 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job tf2-smdataparallel-mrcnn-fsx-3: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /usr/local/bin/python3.7 -m mpi4py DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm.py --amp  --checkpoint /opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_batch_size 8 --eval_samples 5000 --init_learning_rate 0.04 --learning_rate_steps 3750,5000 --mode train --model_dir s3://sagemaker-us-wes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-935a095db72c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Submit SageMaker training job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3640\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3641\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3642\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3220\u001b[0m                 ),\n\u001b[1;32m   3221\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3222\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3223\u001b[0m             )\n\u001b[1;32m   3224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job tf2-smdataparallel-mrcnn-fsx-3: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=sockets -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p3.16xlarge smddprun /usr/local/bin/python3.7 -m mpi4py DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm.py --amp  --checkpoint /opt/ml/input/data/train/model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603 --eval_batch_size 8 --eval_samples 5000 --init_learning_rate 0.04 --learning_rate_steps 3750,5000 --mode train --model_dir s3://sagemaker-us-wes"
     ]
    }
   ],
   "source": [
    "# Submit SageMaker training job\n",
    "estimator.fit(inputs=data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to understand how SageMaker uses Docker to train custom models.\n",
    "* To learn more about using Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms\n",
    "](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html).\n",
    "\n",
    "* To learn more about using Docker to train your own models with Amazon SageMaker, see [Example Notebooks: Use Your Own Algorithm or Model](https://docs.aws.amazon.com/sagemaker/latest/dg/adv-bring-own-examples.html).\n",
    "* To see other examples of distributed training using Amazon SageMaker and TensorFlow, see [Distributed TensorFlow training using Amazon SageMaker\n",
    "](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/distributed_tensorflow_mask_rcnn)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
